# https://shorturl.at/gLubM

---
# ğŸ“˜ **5ë¶€. ë”¥ëŸ¬ë‹ ê¸°ìˆ ê³¼ í™œìš©**

---
#  **6ì¥. ì‹œê³„ì—´ ë°ì´í„°ì™€ ë”¥ëŸ¬ë‹**

---

## **(1) ì‹œê³„ì—´ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë¬¸ì œ**

### âœ… ì‹œê³„ì—´ ë°ì´í„°ë€?

ì‹œê³„ì—´(Time Series) ë°ì´í„°ëŠ” **ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ê¸°ë¡ëœ ê°’ë“¤ì˜ ì§‘í•©**ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë“¤ì´ ì‹œê³„ì—´ì…ë‹ˆë‹¤.

| ì‹œê°„    | ì˜¨ë„(Â°C) | ìŠµë„(%) | ì§„ë™(m/sÂ²) |
| ----- | ------ | ----- | -------- |
| 09:00 | 22.1   | 60.3  | 0.05     |
| 09:10 | 22.3   | 59.8  | 0.07     |
| 09:20 | 22.4   | 59.0  | 0.08     |

ì‹œê³„ì—´ ë°ì´í„°ëŠ” **ê°’ì˜ ìˆœì„œê°€ ì¤‘ìš”**í•˜ë©°, ì˜ˆì¸¡ ëª¨ë¸ì€ ê³¼ê±° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

---

### âœ… ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ì£¼ìš” ìœ í˜•

| ìœ í˜•                | ì„¤ëª…                     | ì˜ˆì‹œ                        |
| ----------------- | ---------------------- | ------------------------- |
| **ë‹¨ì¼ ì…ë ¥ â†’ ë‹¨ì¼ ì¶œë ¥** | ê³¼ê±° 1ê°œ ì‹œì  â†’ ë¯¸ë˜ 1ê°œ ì‹œì  ì˜ˆì¸¡ | í˜„ì¬ ì˜¨ë„ë¡œ 1ì‹œê°„ í›„ ì˜¨ë„ ì˜ˆì¸¡        |
| **ë‹¤ì¤‘ ì…ë ¥ â†’ ë‹¨ì¼ ì¶œë ¥** | ê³¼ê±° ì—¬ëŸ¬ ì‹œì  â†’ ë¯¸ë˜ 1ê°œ ì˜ˆì¸¡    | ìµœê·¼ 24ì‹œê°„ ì˜¨ë„ë¡œ ë‚´ì¼ 09ì‹œ ì˜¨ë„ ì˜ˆì¸¡  |
| **ë‹¤ì¤‘ ì…ë ¥ â†’ ë‹¤ì¤‘ ì¶œë ¥** | ê³¼ê±° ì‹œê³„ì—´ â†’ ë¯¸ë˜ ì‹œê³„ì—´ ì˜ˆì¸¡     | 1ì¼ì¹˜ ì˜¨ë„ë¡œ ë‹¤ìŒ 1ì¼ ì˜ˆì¸¡          |
| **ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ì˜ˆì¸¡**    | ì—¬ëŸ¬ ë³€ìˆ˜ ì‚¬ìš©               | ì˜¨ë„, ìŠµë„, COâ‚‚ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì˜¨ë„ ì˜ˆì¸¡ |

---

### âœ… ì „í†µì  ì‹œê³„ì—´ ì˜ˆì¸¡ vs ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡

| í•­ëª©      | ì „í†µì  ì ‘ê·¼ (ARIMA ë“±) | ë”¥ëŸ¬ë‹ ì ‘ê·¼ (RNN/LSTM ë“±)    |
| ------- | ---------------- | ---------------------- |
| íŠ¹ì§•      | í†µê³„ì , ìˆ˜ì‹ ê¸°ë°˜       | ë°ì´í„° ê¸°ë°˜, í•™ìŠµ ê¸°ë°˜          |
| íŠ¹ì§•ëŸ‰ ì¶”ì¶œ  | í•„ìš”               | ë¶ˆí•„ìš” (end-to-end)       |
| ë¹„ì„ í˜•ì„± ì²˜ë¦¬ | ì–´ë ¤ì›€              | ê°€ëŠ¥                     |
| ë‹¤ë³€ëŸ‰ ì…ë ¥  | ì œí•œì               | ë§¤ìš° ìš©ì´                  |
| ì˜ˆì¸¡ ì •í™•ë„  | ì•ˆì •ì  (ë‹¨ê¸°)         | ìš°ìˆ˜ (ë¹„ì„ í˜•, ì¥ê¸°)           |
| í•™ìŠµëŸ‰     | ì ìŒ               | ë§ìŒ                     |
| ì˜ˆì‹œ      | ARIMA, Prophet   | LSTM, GRU, Transformer |

---

## **(2) AIoTì—ì„œì˜ ì‹œê³„ì—´ ë°ì´í„°**

AIoT(Artificial Intelligence of Things)ëŠ” **IoTì—ì„œ ìˆ˜ì§‘ëœ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§€ëŠ¥í˜• ì˜ˆì¸¡ ë° ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê¸°ìˆ **ì…ë‹ˆë‹¤.

### âœ… ëŒ€í‘œì ì¸ AIoT ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì‹œ

| ì„¼ì„œ ìœ í˜•  | ì˜ˆì¸¡ ëŒ€ìƒ  | ì„¤ëª…                |
| ------ | ------ | ----------------- |
| ì˜¨ë„ ì„¼ì„œ  | ë¯¸ë˜ ì˜¨ë„  | ì˜¨ì‹¤ ë‚´ë¶€ ì˜¨ë„ ì œì–´       |
| COâ‚‚ ì„¼ì„œ | ê¸°ë¥˜ ì´ìƒ  | ê³µê¸° ìˆœí™˜ ë˜ëŠ” ì •ì²´ ì˜ˆì¸¡    |
| ì§„ë™ ì„¼ì„œ  | ê¸°ê³„ ê³ ì¥  | ì´ìƒ ì§„ë™ ê°ì§€ ë° ê²½ê³      |
| ì „ë ¥ ì„¼ì„œ  | ì—ë„ˆì§€ ì†Œë¹„ | ì†Œë¹„ëŸ‰ ê¸‰ì¦ ì˜ˆì¸¡ ë° ìë™ ì ˆì „ |

---

### âœ… ì‹¤ì‹œê°„ AIoT ë°ì´í„° íë¦„

```text
[ì„¼ì„œ ì¸¡ì •] â†’ [ì‹œê³„ì—´ ì €ì¥] â†’ [ìŠ¬ë¼ì´ë”© ìœˆë„ìš°] â†’ [ë”¥ëŸ¬ë‹ ëª¨ë¸ ì…ë ¥] â†’ [ë¯¸ë˜ê°’ ì˜ˆì¸¡] â†’ [ì œì–´]
```

ì˜ˆ:

* 10ë¶„ ê°„ê²©ìœ¼ë¡œ ì˜¨ë„ ì„¼ì„œ ì¸¡ì •
* 24ì‹œê°„(144í¬ì¸íŠ¸)ì„ í•˜ë‚˜ì˜ ìœˆë„ìš°ë¡œ ë¬¶ìŒ
* ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë‹¤ìŒ 10ë¶„ í›„ ì˜¨ë„ë¥¼ ì˜ˆì¸¡
* ì˜¨ë„ê°€ ê¸‰ê²©íˆ ì˜¬ë¼ê°ˆ ê²½ìš° ëƒ‰ë°©ê¸° ìë™ ì‘ë™

---


## **(3) ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸: ìˆœí™˜ ì‹ ê²½ë§(RNN)**

---

### 1) RNN

ê¸°ì¡´ì˜ ì™„ì „ ì—°ê²° ì‹ ê²½ë§(Dense Layer)ì€ ì…ë ¥ì˜ ìˆœì„œ ì •ë³´ë‚˜ ì‹œê°„ì  ë§¥ë½ì„ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ **ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìì—°ì–´ì²˜ëŸ¼ ê³¼ê±°ì˜ ìƒíƒœê°€ í˜„ì¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë°ì´í„°**ì—ì„œëŠ” ì‹œê°„ì ì¸ ì˜ì¡´ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Network, RNN)** ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. RNNì€ **ì´ì „ ì‹œì ì˜ ì¶œë ¥ì„ í˜„ì¬ ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ìˆœí™˜ì ìœ¼ë¡œ ì „ë‹¬**í•¨ìœ¼ë¡œì¨ **ì‹œê°„ì˜ íë¦„ì„ ëª¨ë¸ë§**í•©ë‹ˆë‹¤.

- https://en.wikipedia.org/wiki/Recurrent_neural_network
---

#### âœ…  RNNì˜ ê¸°ë³¸ êµ¬ì¡°
- https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg

- https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:RNN_architecture.png

RNNì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤:

* ì…ë ¥: $x^{(t)}$ (ì‹œì  $t$ì˜ ì…ë ¥)
* ì€ë‹‰ ìƒíƒœ: $h^{(t)}$ (ì´ì „ ì •ë³´ê°€ ëˆ„ì ëœ ìƒíƒœ)
* ì¶œë ¥: $y^{(t)}$

```text
x(1) â”€â”€â–º[RNN Cell]â”€â”€â–º h(1)
           â”‚
x(2) â”€â”€â–º[RNN Cell]â”€â”€â–º h(2)
           â”‚
...     (ë°˜ë³µ êµ¬ì¡°)
```



#### âœ… ì‹œê°„ ì „ê°œ(Unfolding) êµ¬ì¡°

RNNì€ **í•˜ë‚˜ì˜ RNN ì…€ì„ ì—¬ëŸ¬ ì‹œê°„ì¶•ìœ¼ë¡œ ë³µì‚¬**í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë¥¼ **ì‹œê°„ì— ë”°ë¼ í¼ì¹œë‹¤(unfold)** ê³  í‘œí˜„í•©ë‹ˆë‹¤.

```text
x(1) â†’ h(1)
        â†“
x(2) â†’ h(2)
        â†“
x(3) â†’ h(3)
```

ê° ì‹œì ì˜ ì€ë‹‰ ìƒíƒœëŠ” ì´ì „ ì‹œì ì˜ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:

$$
h^{(t)} = \tanh(W_h h^{(t-1)} + W_x x^{(t)} + b)
$$

* $W_h$: ì€ë‹‰ ìƒíƒœì—ì„œì˜ ê°€ì¤‘ì¹˜
* $W_x$: ì…ë ¥ ê°€ì¤‘ì¹˜
* $\tanh$: í™œì„±í™” í•¨ìˆ˜ (ê¸°ë³¸ RNNì˜ ê²½ìš°)

---

#### âœ… RNN ì—°ì‚° íë¦„

| ì‹œì     | ê³„ì‚° íë¦„                                            | ì¶œë ¥                      |
| ----- | ------------------------------------------------ | ----------------------- |
| $t=1$ | $h^{(1)} = \tanh(W_x x^{(1)} + W_h h^{(0)} + b)$ | $y^{(1)} = W_y h^{(1)}$ |
| $t=2$ | $h^{(2)} = \tanh(W_x x^{(2)} + W_h h^{(1)} + b)$ | $y^{(2)} = W_y h^{(2)}$ |
| ...   | ...                                              | ...                     |

---

#### âœ… RNNì˜ í•œê³„: ê¸°ìš¸ê¸° ì†Œì‹¤ê³¼ í­ë°œ

ê¸°ì¡´ RNNì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œì ì´ ì¡´ì¬í•©ë‹ˆë‹¤:

| ë¬¸ì œ                              | ì„¤ëª…                                     |
| ------------------------------- | -------------------------------------- |
| **ê¸°ìš¸ê¸° ì†Œì‹¤ (Vanishing Gradient)** | ì—­ì „íŒŒ ì¤‘ ê¸°ìš¸ê¸°ê°€ ì ì  ì‘ì•„ì ¸ **ì¥ê¸° ì˜ì¡´ ê´€ê³„ í•™ìŠµì´ ì–´ë ¤ì›€** |
| **ê¸°ìš¸ê¸° í­ë°œ (Exploding Gradient)** | ê¸°ìš¸ê¸°ê°€ ê³„ì† ì»¤ì ¸ **í•™ìŠµì´ ë°œì‚°í•˜ê±°ë‚˜ ë¶ˆì•ˆì •í•´ì§**         |
| **ì¥ê¸° ê¸°ì–µë ¥ ë¶€ì¡±**                   | ì •ë³´ê°€ ì‹œì ì´ ì§€ë‚¨ì— ë”°ë¼ í¬ë¯¸í•´ì ¸ ì¥ê¸° ì˜ˆì¸¡ì´ ì–´ë ¤ì›€         |

ì´ëŸ¬í•œ í•œê³„ëŠ” ì´í›„ì— ë“±ì¥í•  **LSTMê³¼ GRU** ëª¨ë¸ì˜ ë™ê¸°ì´ì ì´ìœ ê°€ ë©ë‹ˆë‹¤.

---

### **(2) LSTM(Long Short-Term Memory)**


ê¸°ì¡´ RNNì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ**(long-term dependency)ë¥¼ í•™ìŠµí•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì•ë¶€ë¶„ì˜ ì •ë³´ê°€ ë’·ë¶€ë¶„ ì¶œë ¥ì— ì˜í–¥ì„ ì£¼ì–´ì•¼ í•  ê²½ìš°, **ê¸°ìš¸ê¸° ì†Œì‹¤** ë¬¸ì œë¡œ ì¸í•´ í•™ìŠµì´ ë˜ì§€ ì•Šê±°ë‚˜ ë§¤ìš° ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **LSTM (Long Short-Term Memory)** êµ¬ì¡°ê°€ ë“±ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.
LSTMì€ **ì •ë³´ë¥¼ ì¥ê¸°ì ìœ¼ë¡œ ìœ ì§€í•˜ê³  í•„ìš”í•  ë•Œë§Œ ìŠëŠ” ë©”ì»¤ë‹ˆì¦˜**ì„ ê°–ì¶˜ RNNì˜ í™•ì¥ êµ¬ì¡°ì…ë‹ˆë‹¤.

---

#### âœ… LSTM ì…€ì˜ êµ¬ì¡°

- https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Long_Short-Term_Memory.svg

```text
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 x_t â†’ â”‚ ì…ë ¥ ê²Œì´íŠ¸ i_t â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                        â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 h_{t-1} â†’ â”€â”€â”€â”€â”€â”€â–º â”‚ ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸ (C_t) â”œâ”€â”€â”€â–º h_t (ì¶œë ¥)
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–²
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
       â”‚ ë§ê° ê²Œì´íŠ¸ f_t â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

LSTMì€ ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ê²Œì´íŠ¸(gate)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤:

| ê²Œì´íŠ¸ ì´ë¦„           | ì—­í•                  | ìˆ˜ì‹                                          |
| ---------------- | ------------------ | ------------------------------------------- |
| **ì…ë ¥ ê²Œì´íŠ¸** $i_t$ | í˜„ì¬ ì…ë ¥ì„ ê¸°ì–µí• ì§€ ê²°ì •     | $i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$ |
| **ë§ê° ê²Œì´íŠ¸** $f_t$ | ê³¼ê±° ê¸°ì–µì„ ìœ ì§€í• ì§€ ì§€ìš¸ì§€ ê²°ì • | $f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$ |
| **ì¶œë ¥ ê²Œì´íŠ¸** $o_t$ | ì…€ ìƒíƒœë¥¼ ì¶œë ¥í• ì§€ ê²°ì •      | $o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)$ |

ì´ ì„¸ ê²Œì´íŠ¸ë¥¼ í†µí•´ ì…€ ìƒíƒœ(Cell State, $C_t$)ë¥¼ ì¡°ì ˆí•˜ê³ , ì€ë‹‰ ìƒíƒœ($h_t$)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

---

#### âœ… LSTM ì…€ì˜ ì „ì²´ ë™ì‘ íë¦„

1. **ë§ê°(forgot)**: ì´ì „ ì…€ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ ê²°ì •

   $$
   f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
   $$

2. **ì…ë ¥(input)**: í˜„ì¬ ì…ë ¥ì„ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ê²°ì •

   $$
   i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
   $$

   $$
   \tilde{C}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
   $$

3. **ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸**:

   $$
   C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
   $$

4. **ì¶œë ¥(output)**: ë‹¤ìŒ ì€ë‹‰ ìƒíƒœ ê²°ì •

   $$
   o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
   $$

   $$
   h_t = o_t \odot \tanh(C_t)
   $$

> ì—¬ê¸°ì„œ $\odot$ëŠ” element-wise ê³±, $\sigma$ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì…ë‹ˆë‹¤.

---

#### âœ… LSTM ë‚´ë¶€ êµ¬ì¡°



---

#### âœ… LSTMì˜ ì¥ì ê³¼ íŠ¹ì§•

| í•­ëª©               | ì„¤ëª…                               |
| ---------------- | -------------------------------- |
| **ì¥ê¸° ê¸°ì–µë ¥**       | ì…€ ìƒíƒœë¥¼ í†µí•´ ì •ë³´ ë³´ì¡´ ê°€ëŠ¥                |
| **ìœ ì—°í•œ ì •ë³´ íë¦„ ì œì–´** | ê²Œì´íŠ¸ë¥¼ í†µí•´ ì…ë ¥/ì¶œë ¥/ê¸°ì–µì„ ì„ íƒì  ì œì–´         |
| **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°** | ì•ˆì •ì ì¸ ì—­ì „íŒŒ ê°€ëŠ¥                      |
| **í•™ìŠµ ì•ˆì •ì„±**       | RNNë³´ë‹¤ í•™ìŠµ ìˆ˜ë ´ë¥  ë†’ìŒ                  |
| **ëŒ€í‘œì  ì‚¬ìš© ë¶„ì•¼**    | ìŒì„± ì¸ì‹, ê¸°ê³„ ê³ ì¥ ì˜ˆì¸¡, ë‚ ì”¨ ì˜ˆì¸¡, ì‹œê³„ì—´ ì˜ˆì¸¡ ë“± |

---

### **(3) GRU(Gated Recurrent Unit)ì˜ êµ¬ì¡°ì™€ íŠ¹ì§•**

\*\*GRU(Gated Recurrent Unit)\*\*ëŠ” 2014ë…„ Cho et al.ì´ LSTMì˜ ëŒ€ì•ˆìœ¼ë¡œ ì œì•ˆí•œ ìˆœí™˜ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤.
LSTMì´ ì¥ê¸° ê¸°ì–µì„ ìœ ì§€í•˜ëŠ” ë° íš¨ê³¼ì ì´ì§€ë§Œ, êµ¬ì¡°ê°€ ë³µì¡í•˜ê³  íŒŒë¼ë¯¸í„°ê°€ ë§ë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆìŠµë‹ˆë‹¤.
GRUëŠ” **LSTMì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ê²Œì´íŠ¸ ìˆ˜ë¥¼ ì¤„ì´ê³  êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”**í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

---

#### âœ… GRUì˜ ì…€ êµ¬ì¡°

GRUëŠ” ë‘ ê°€ì§€ ì£¼ìš” ê²Œì´íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

- https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Gated_Recurrent_Unit.svg


```text
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
x_t â”€â”€â”€â–ºâ”€â”€â”‚        ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ z_t     â”œâ”€â”€â”€â”€â”€â”€â”
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                          â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    ê°€ì¤‘ í‰ê· 
h_{t-1}â”€â”€â–ºâ”‚        ë¦¬ì…‹ ê²Œì´íŠ¸ r_t         â”œâ”€â”€â”€â–º  h_t
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â–²
                                          â”‚
    x_t + (r_t * h_{t-1}) â†’  tanh â†’  â”€â”€â”€â”€â”€â”˜
                 = \tilde{h}_t
```

| ê²Œì´íŠ¸                | ì„¤ëª…                 | ìˆ˜ì‹                                    |
| ------------------ | ------------------ | ------------------------------------- |
| **ë¦¬ì…‹ ê²Œì´íŠ¸** $r_t$   | ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë¬´ì‹œí• ì§€ ê²°ì • | $r_t = \sigma(W_r x_t + U_r h_{t-1})$ |
| **ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸** $z_t$ | ì´ì „ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ ê²°ì • | $z_t = \sigma(W_z x_t + U_z h_{t-1})$ |

ê·¸ë¦¬ê³  ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ $\tilde{h}_t$ ë¥¼ ê³„ì‚°í•˜ì—¬, ìµœì¢… ì€ë‹‰ ìƒíƒœ $h_t$ ë¥¼ ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±í•©ë‹ˆë‹¤:

$$
\tilde{h}_t = \tanh(W_h x_t + U_h (r_t \odot h_{t-1}))
$$

$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

---

#### âœ… GRUì˜ ì—°ì‚° íë¦„

| ë‹¨ê³„                     | ì„¤ëª…                      |
| ---------------------- | ----------------------- |
| â‘  ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ $z_t$       | í˜„ì¬ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìƒˆë¡œ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì • |
| â‘¡ ë¦¬ì…‹ ê²Œì´íŠ¸ $r_t$         | ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ê²°ì •      |
| â‘¢ ìƒˆë¡œìš´ ìƒíƒœ $\tilde{h}_t$ | í˜„ì¬ ì…ë ¥ê³¼ ë¦¬ì…‹ëœ ì´ì „ ìƒíƒœë¡œ ê³„ì‚°    |
| â‘£ ìµœì¢… ìƒíƒœ $h_t$          | ì´ì „ ìƒíƒœì™€ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ê²Œì´íŠ¸ë¡œ í˜¼í•©  |

---

#### âœ… GRU vs LSTM ë¹„êµ

| í•­ëª©     | GRU               | LSTM            |
| ------ | ----------------- | --------------- |
| ê²Œì´íŠ¸ ìˆ˜  | 2ê°œ (ì—…ë°ì´íŠ¸, ë¦¬ì…‹)     | 3ê°œ (ì…ë ¥, ë§ê°, ì¶œë ¥) |
| ì…€ ìƒíƒœ   | ì—†ìŒ (ì€ë‹‰ ìƒíƒœë§Œ ì‚¬ìš©)    | ì…€ ìƒíƒœì™€ ì€ë‹‰ ìƒíƒœ ë¶„ë¦¬  |
| íŒŒë¼ë¯¸í„° ìˆ˜ | ì ìŒ                | ë§ìŒ              |
| ê³„ì‚° ì†ë„  | ë¹ ë¦„                | ëŠë¦¼              |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | ì ìŒ                | ë§ìŒ              |
| í‘œí˜„ë ¥    | ì¶©ë¶„ (ì‹¤ë¬´ì—ì„œë„ ë„ë¦¬ ì‚¬ìš©ë¨) | ë§¤ìš° ê°•ë ¥           |
| í•™ìŠµ ì•ˆì •ì„± | ë³´í†µ                | ë§¤ìš° ì•ˆì •ì           |



---

#### âœ… ëª¨ë¸ ì„ íƒ ê¸°ì¤€
ë‹¤ìŒì€ ì‹œê³„ì—´ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ ëª¨ë¸ì„ ì„ íƒí•  ë•Œ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì…ë‹ˆë‹¤:

| ê³ ë ¤ ê¸°ì¤€                 | ì¶”ì²œ ëª¨ë¸      |
| --------------------- | ---------- |
| ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì§§ê³  ê³„ì‚° ìì›ì´ ì œí•œë¨ | RNN ë˜ëŠ” GRU |
| ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì´ ì¤‘ìš”í•¨        | LSTM       |
| ì‹¤ì‹œê°„ ì²˜ë¦¬ ë° ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ í™˜ê²½  | GRU        |
| í•™ìŠµ ì•ˆì •ì„±ê³¼ ê³ ì„±ëŠ¥ì´ í•„ìˆ˜       | LSTM       |
| í•™ìŠµ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ê°€ ë¯¼ê°í•œ ìƒí™©    | GRU        |
| ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë‹¨ìˆœ ì˜ˆì¸¡      | RNN        |

---
#### âœ… ì‹¤ë¬´ ì ìš© ì˜ˆì‹œ

| ë¶„ì•¼                     | ì ìš© ëª¨ë¸       | ì´ìœ                   |
| ---------------------- | ----------- | ------------------- |
| ì˜ˆ: AIoT ì„¼ì„œ ë°ì´í„° ì˜ˆì¸¡      | GRU         | ì—°ì‚°ëŸ‰ì´ ì ê³  ì‹¤ì‹œê°„ ë°˜ì‘ì— ì í•©  |
| ì˜ˆ: ê³ ì¥ ì§„ë‹¨ ì˜ˆì¸¡ (ìˆ˜ë°± ì‹œê°„ ê¸°ì¤€) | LSTM        | ì¥ê¸° ì˜ì¡´ ê´€ê³„ í•„ìš”         |
| ì˜ˆ: ì˜¨ë„/ìŠµë„ ë‹¨ê¸° ì˜ˆì¸¡         | RNN ë˜ëŠ” GRU  | êµ¬ì¡° ë‹¨ìˆœí•˜ê³  ì¶©ë¶„í•œ ì„±ëŠ¥      |
| ì˜ˆ: ECG ë“± ìƒì²´ì‹ í˜¸ ì‹œê³„ì—´ ë¶„ì„   | LSTM ë˜ëŠ” GRU | íŠ¹ì§• ê°„ ìƒê´€ì„± ë³´ì¡´, ì•ˆì •ì„± ì¤‘ìš” |
| ì˜ˆ: ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì • ì¶”ë¡   | GRU         | ê²½ëŸ‰, ë¹ ë¥¸ ì²˜ë¦¬ í•„ìš”        |

---
## ğŸ“˜ **(4) LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤ìŠµ**



### âœ…ETTh1 ë°ì´í„°ì…‹

**ETTh1 (Electricity Transformer Temperature - Hourly 1)** ë°ì´í„°ì…‹ì€ ì „ë ¥ ë³€ì••ê¸° ì‹œìŠ¤í…œì˜ ì‹œê³„ì—´ ë™ì‘ ë°ì´í„°ë¥¼ ê¸°ë¡í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ, **ë‹¤ì¤‘ ë³€ìˆ˜ ì‹œê³„ì—´ ì˜ˆì¸¡(Multivariate Time Series Forecasting)** ë¬¸ì œì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.


#### ë°ì´í„°ì…‹ ìš”ì•½

| í•­ëª©       | ë‚´ìš©                            |
| -------- | ----------------------------- |
| ì´ë¦„       | ETTh1.csv                     |
| ì‹œê°„ ê°„ê²©    | 1ì‹œê°„                           |
| ê¸°ê°„       | ì•½ 1ë…„ì¹˜                         |
| ìƒ˜í”Œ ìˆ˜     | ì•½ 8,000ê°œ                      |
| ì£¼ìš” ëª©ì     | ì‹œê³„ì—´ ì˜ˆì¸¡ (ex. í–¥í›„ T1 ì˜ˆì¸¡)         |
| ì£¼ìš” íƒ€ê¹ƒ ë³€ìˆ˜ | `OT` (ìœ ì¶œ ì˜¨ë„: Oil Temperature) |

---

#### Features

| Feature ì´ë¦„ | ì„¤ëª…                                      | ë‹¨ìœ„ |
| ---------- | --------------------------------------- | -- |
| **date**   | ì‹œê°„ ì •ë³´ (YYYY-MM-DD HH)                   | -  |
| **HUFL**   | ê³ ì£¼íŒŒ ìœ ë„ ë¶€í•˜ (High Usage Frequency Load)   | kW |
| **HULL**   | ì €ì£¼íŒŒ ìœ ë„ ë¶€í•˜ (Low Usage Frequency Load)    | kW |
| **MUFL**   | ì¤‘ê°„ ì£¼íŒŒìˆ˜ ë¶€í•˜ (Medium Usage Frequency Load) | kW |
| **MULL**   | ì¤‘ê°„ ì£¼íŒŒìˆ˜ ì €ë¶€í•˜ (Medium Usage Low Load)      | kW |
| **LUFL**   | ì €ì£¼íŒŒ ê³ ë¶€í•˜ (Low Usage Frequency Load)      | kW |
| **LULL**   | ì €ì£¼íŒŒ ì €ë¶€í•˜ (Low Usage Low Load)            | kW |
| **OT**     | ë³€ì••ê¸° ì˜¤ì¼ ì˜¨ë„ (Oil Temperature) â†’ ì˜ˆì¸¡ ëŒ€ìƒ     | Â°C |

> ì°¸ê³ : HUFL, MUFL, LUFL ë“±ì€ ë³€ì „ì†Œë‚˜ ì†¡ì „ ì‹œìŠ¤í…œì—ì„œ ì¸¡ì •ëœ **ë¶€í•˜(Load)** ë˜ëŠ” **ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰**ì„ ì‹œê³„ì—´ì ìœ¼ë¡œ ì¸¡ì •í•œ ê°’ì…ë‹ˆë‹¤.

---
### âœ… ì‹¤ìŠµ ëª©í‘œ

* **ETTh1 ë°ì´í„°ì…‹ (Electricity Transformer Temperature for 1-hour level)**
* ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì•ìœ¼ë¡œ 24ì‹œê°„ í›„ì˜ OT ì˜ˆì¸¡
* ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ë”¥ëŸ¬ë‹ ì…ë ¥ í˜•ì‹ì— ë§ì¶¤
* ëª¨ë¸ ì„±ëŠ¥ì„ mse, r2-scoreë¡œ í‰ê°€


---

### âœ…  ì‹¤ìŠµ ì½”ë“œ

```python

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

 # 2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
url = "https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv"
filename = "ETTh1.csv"
urllib.request.urlretrieve(url, filename)

df = pd.read_csv(filename)
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# 3. ë‹¨ì¼ í”¼ì²˜ ì„ íƒ (ì˜ˆ: 'OT')
feature = 'OT'
data = df[[feature]]


# 4. ì‹œê³„ì—´ ì „ì²´ ì‹œê°í™”
plt.figure(figsize=(14, 4))
plt.plot(data.index[:500], data[feature].values[:500])
plt.title(f"ETTh1 Time Series - {feature}")
plt.xlabel("Time")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()

# 5. ì •ê·œí™”
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# 6. ì‹œí€€ìŠ¤ ìƒì„±
def create_sequences(data, window_size):
    xs, ys = [], []
    for i in range(len(data) - window_size):
        x = data[i:i + window_size]
        y = data[i + window_size]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

window_size = 24
X, y = create_sequences(data_scaled, window_size)

# 7. ì‹œí€€ìŠ¤ ìƒ˜í”Œ 10ê°œ ì‹œê°í™”
fig, axs = plt.subplots(5, 2, figsize=(14, 10))
axs = axs.ravel()
for i in range(10):
    axs[i].plot(X[i].flatten(), label="X")
    axs[i].axhline(y=y[i], color='red', linestyle='--', label="y")
    axs[i].set_title(f"Sequence Sample {i}")
    axs[i].legend()
    axs[i].grid(True)
plt.tight_layout()
plt.show()

# 8. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# 9. LSTM ëª¨ë¸ ì •ì˜ (xLSTM ìŠ¤íƒ€ì¼)
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(window_size, 1)),
    Dropout(0.2),
    LSTM(32),
    Dropout(0.2),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')


# 10. í•™ìŠµ
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# 11. ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 12. ê²°ê³¼ ë³µì› ë° í‰ê°€
y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))
y_pred_inv = scaler.inverse_transform(y_pred)

# ì‹œê°í™”
plt.figure(figsize=(12, 5))
plt.plot(y_test_inv[:300], label='True')
plt.plot(y_pred_inv[:300], label='Predicted')
plt.title("LSTM Forecast on ETTh1 (OT)")
plt.xlabel("Time Step")
plt.ylabel("Temperature")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# í‰ê°€ ì§€í‘œ
mse = mean_squared_error(y_test_inv, y_pred_inv)
r2 = r2_score(y_test_inv, y_pred_inv)
print(f"MSE: {mse:.4f}, RÂ²: {r2:.4f}")

```
---

## ğŸ§¾ ì½”ë“œ ì£¼ìš” í•¨ìˆ˜ ì„¤ëª…

| í•¨ìˆ˜ëª…                                   | ì—­í•                     | ë¹„ê³                       |
| ------------------------------------- | --------------------- | ----------------------- |
| `create_sequences(X, y, window_size)` | ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì…ë ¥/íƒ€ê¹ƒ ìŒìœ¼ë¡œ ë³€í™˜ | LSTMìš© ì‹œê³„ì—´ ìœˆë„ìš° êµ¬ì„±        |
| `train_test_split()`                  | í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬         | sklearn ì‚¬ìš©              |
| `StandardScaler().fit_transform()`    | ì •ê·œí™”                   | í‰ê·  0, í‘œì¤€í¸ì°¨ 1            |
| `Sequential([...])`                   | CNN-LSTM ëª¨ë¸ êµ¬ì„±        | Conv1D â†’ MaxPool â†’ LSTM |
| `model.fit()`                         | ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰              | Epoch=10                |

---
