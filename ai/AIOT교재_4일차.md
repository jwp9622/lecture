<<<<<<< HEAD
# - https://shorturl.at/aLUfI (ëŒ€ë¬¸ì i)

# - https://shorturl.at/3nQFr
--- 
# ğŸ“˜ **5ë¶€. ë”¥ëŸ¬ë‹ ê¸°ìˆ ê³¼ í™œìš©**
---

# ğŸ“˜ **1ì¥. ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ê°œìš”ì™€ êµ¬ì¡°**



ì‹¬ì¸µ ì‹ ê²½ë§(Deep Neural Network, DNN)ì€ ê¸°ì¡´ì˜ ì¸ê³µì‹ ê²½ë§(ANN, Artificial Neural Network)ì—ì„œ ë°œì „í•œ êµ¬ì¡°ë¡œ, 2006ë…„ Geoffrey Hintonê³¼ Yoshua Bengio ë“±ì´ ì œì•ˆí•œ **ë”¥ëŸ¬ë‹(Deep Learning)** ê°œë…ì´ ë„ë¦¬ í¼ì§€ë©´ì„œ ì£¼ëª©ë°›ê¸° ì‹œì‘í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ˆê¸°ì—ëŠ” XOR ë¬¸ì œ í•´ê²°ì´ë‚˜ ìŒì„± ì¸ì‹ ë“±ì— í™œìš©ë˜ì—ˆì§€ë§Œ, **2012ë…„ ILSVRCì—ì„œ AlexNetì´ ëŒ€íšŒ ìš°ìŠ¹**ì„ í•˜ë©´ì„œ ì´ë¯¸ì§€, ìŒì„±, ì‹œê³„ì—´ ë°ì´í„° ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ DNNì´ í‘œì¤€ìœ¼ë¡œ ìë¦¬ ì¡ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
- [ìœ„í‚¤í”¼ë””ì•„:ì‹¬ì¸µì‹ ê²½ë§](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))
- [ìœ„í‚¤ë°±ê³¼: ë”¥ëŸ¬ë‹](https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D)
- [ìœ„í‚¤ë…ìŠ¤: deep neural network](https://wikidocs.net/120152)
---


## 1. ì¸ê³µì‹ ê²½ë§(ANN)ì˜ êµ¬ì¡° 

| ê³„ì¸µ ìœ í˜•             | ì„¤ëª…         |
| ----------------- | ---------- |
| ì…ë ¥ì¸µ(Input Layer)  | ë°ì´í„° ì…ë ¥ì„ ë‹´ë‹¹ |
| ì€ë‹‰ì¸µ(Hidden Layer) | ë‚´ë¶€ íŠ¹ì§• ì¶”ì¶œ   |
| ì¶œë ¥ì¸µ(Output Layer) | ìµœì¢… ì˜ˆì¸¡ê°’ ìƒì„±  |

## 2. ì‹¬ì¸µ ì‹ ê²½ë§(DNN)ì´ë€?

* ì€ë‹‰ì¸µì´ **2ê°œ ì´ìƒ**ì¸ ANN êµ¬ì¡°ë¥¼ DNNì´ë¼ í•©ë‹ˆë‹¤.
* ê° ê³„ì¸µì€ **ê°€ì¤‘ì¹˜(weight)**, **í¸í–¥(bias)**, \*\*í™œì„±í™” í•¨ìˆ˜(activation)\*\*ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

## 3. ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ ë¹„êµ

| í•¨ìˆ˜      | ìˆ˜ì‹                                  | íŠ¹ì§•                    |
| ------- | ----------------------------------- | --------------------- |
| ReLU    | $f(x) = \max(0, x)$                 | ë¹ ë¥¸ í•™ìŠµ, ìŒìˆ˜ì—ì„œ ì£½ëŠ” ë‰´ëŸ° ê°€ëŠ¥ì„± |
| Sigmoid | $\frac{1}{1 + e^{-x}}$              | ì¶œë ¥ 0\~1, ê²½ì‚¬ ì†Œì‹¤ ë¬¸ì œ ì¡´ì¬  |
| Tanh    | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | -1\~1 ì¶œë ¥, ì¤‘ì‹¬í™”ëœ ì¶œë ¥     |

---

## 4. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multilayer Perceptron, MLP)ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ 3ê°€ì§€ë¥¼ ê¸°ìˆ í•˜ì‹œì˜¤.

ğŸŸ© **ì •ë‹µ:** ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µ

ğŸ“ **ì„¤ëª…:** MLPëŠ” ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì€ë‹‰ì¸µê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ í¬í•¨í•´ì•¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ë¬¸ì œ 2. ReLU í•¨ìˆ˜ì˜ ì£¼ìš” ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** ìŒìˆ˜ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ì´ 0ì´ ë˜ì–´ í•™ìŠµì´ ë©ˆì¶œ ìˆ˜ ìˆìŒ (ì£½ì€ ë‰´ëŸ° ë¬¸ì œ)

ğŸ“ **ì„¤ëª…:** ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Leaky ReLU, ELU ë“±ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

---

## 5. ì‹¤ìŠµ: Ai4I 2020 Datasetì„ í™œìš©í•œ MLP ë¶„ë¥˜


| ë‹¨ê³„         | ë‚´ìš©                                    |
| ---------- | ------------------------------------- |
| **ì…ë ¥ ë°ì´í„°** | Type, ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, ë§ˆëª¨ ì‹œê°„            |
| **ì¶œë ¥ (y)** | ê³ ì¥(1), ì •ìƒ (0)|
| **ëª¨ë¸ êµ¬ì¡°**  | 64 â†’ 32 â†’ 1 (sigmoid)                |
| **ì†ì‹¤ í•¨ìˆ˜**  | `binary_crossentropy` (ë‹¤ì¤‘ ì´ì§„ íƒ€ê²Ÿìš©)     |



```python
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

# 3. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ì¶”ì¶œ
X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
y = df['Machine failure']

# 4. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. MLP ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. í•™ìŠµ ìˆ˜í–‰
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# 7. í‰ê°€
y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

# 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title("Training History")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **2ì¥. í•™ìŠµê³¼ ìµœì í™” ì•Œê³ ë¦¬ì¦˜**



ì´ˆê¸° ì‹ ê²½ë§ í•™ìŠµ ë°©ì‹ì€ ë‹¨ìˆœí•œ \*\*ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)\*\*ì´ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸µì´ ê¹Šì–´ì§€ë©´ì„œ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” **ê¸°ìš¸ê¸° ì†Œì‹¤(vanishing gradient)** ë¬¸ì œê°€ ë°œìƒí–ˆê³ , 1986ë…„ Rumelhart, Hinton, Williamsê°€ ì œì•ˆí•œ **ì—­ì „íŒŒ(Backpropagation)** ì•Œê³ ë¦¬ì¦˜ì´ ì´ë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.
ì´í›„ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ í–¥ìƒì„ ìœ„í•´ ë‹¤ì–‘í•œ \*\*ìµœì í™” ì•Œê³ ë¦¬ì¦˜(SGD, Momentum, RMSProp, Adam ë“±)\*\*ì´ ì œì•ˆë˜ë©°, ë”¥ëŸ¬ë‹ì˜ í­ë°œì  ë°œì „ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.

---


## 1. ìˆœì „íŒŒ(Forward Propagation)

* ì…ë ¥ ë°ì´í„°ë¥¼ ê³„ì¸µì„ ë”°ë¼ \*\*ì„ í˜• ë³€í™˜(Wx + b)\*\*ê³¼ **ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜**ë¥¼ í†µí•´ ì¶œë ¥ê¹Œì§€ ì „ë‹¬í•©ë‹ˆë‹¤.
* ì˜ˆì‹œ:

  $$
  z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \quad,\quad a^{(l)} = \text{ReLU}(z^{(l)})
  $$

---

## 2. ì†ì‹¤ í•¨ìˆ˜(Loss Function)

* **ë¶„ë¥˜:** Binary Cross Entropy

  $$
  \mathcal{L} = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
  $$
* **íšŒê·€:** MSE, MAE ì‚¬ìš©

---

## 3. ì—­ì „íŒŒ(Backpropagation)

* ì†ì‹¤ í•¨ìˆ˜ì˜ ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ **ì²´ì¸ë£°ë¡œ ë¯¸ë¶„**í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
* ê° ê³„ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë‹¤ìŒ ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

  $$
  W := W - \eta \cdot \frac{\partial \mathcal{L}}{\partial W}
  $$

  $$
  b := b - \eta \cdot \frac{\partial \mathcal{L}}{\partial b}
  $$

---

## 4. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜     | ì„¤ëª…                 | íŠ¹ì§•        |
| -------- | ------------------ | --------- |
| SGD      | í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•          | ë¶ˆì•ˆì •í•˜ì§€ë§Œ ë¹ ë¦„ |
| Momentum | ì†ë„ ê°œë… ë„ì…           | ì§„ë™ ì–µì œ     |
| RMSProp  | ìµœê·¼ ê¸°ìš¸ê¸° ì œê³± í‰ê·        | í•™ìŠµë¥  ìë™ ì¡°ì ˆ |
| Adam     | Momentum + RMSProp | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨ |

---

## 5. í•™ìŠµ ê´€ë ¨ ìš©ì–´

| ìš©ì–´            | ì •ì˜                     |
| ------------- | ---------------------- |
| Batch         | í•œ ë²ˆ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œ ìˆ˜      |
| Epoch         | ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ ëª¨ë‘ í•™ìŠµ      |
| Learning Rate | ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ í­             |
| ì •ê·œí™”           | ê³¼ì í•© ë°©ì§€ (L2, Dropout ë“±) |

---

## 6. ì‹¤ìŠµ: Optimizerì— ë”°ë¥¸ í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (Ai4I 2020 Dataset)

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ë™ì¼í•˜ë¯€ë¡œ ì•„ë˜ì„œë¶€í„° ìˆ˜í–‰

# ëª¨ë¸ ìƒì„± í•¨ìˆ˜
def create_model(optimizer):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ìµœì í™”ê¸° ë³„ í•™ìŠµ ê²°ê³¼ ì €ì¥
optimizers = {'SGD': SGD(), 'RMSprop': RMSprop(), 'Adam': Adam()}
histories = {}

for name, opt in optimizers.items():
    print(f"\n[ {name} Optimizer í•™ìŠµ ì‹œì‘ ]")
    model = create_model(opt)
    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0)
    histories[name] = history
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    print(f"{name} Optimizer ì„±ëŠ¥:\n", classification_report(y_test, y_pred))

# ì •í™•ë„ ì‹œê°í™”
plt.figure(figsize=(10, 5))
for name, history in histories.items():
    plt.plot(history.history['val_accuracy'], label=f'{name} Val Acc')
plt.title("Optimizerë³„ Validation Accuracy ë¹„êµ")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **3ì¥. ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì „ëµ**


ì´ˆê¸° ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ **í•™ìŠµì´ ëŠë¦¬ê³  ë¶ˆì•ˆì •**í•˜ë©°, **ê³¼ì í•©(overfitting)** ë¬¸ì œê°€ ìì£¼ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2010ë…„ëŒ€ ì´ˆë°˜ ë‹¤ì–‘í•œ ê¸°ìˆ ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.

* **Dropout**(2014, Hinton)ì€ ì€ë‹‰ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ê³ ,
* **Batch Normalization**(2015, Ioffe & Szegedy)ì€ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ë¥¼ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
* ë˜í•œ, **Early Stopping**, **Regularization**, **Hyperparameter Tuning** ë“±ë„ ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ì „ëµìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.

---


## 1. ì´ˆê¸°í™” ì „ëµ (Weight Initialization)

| ë°©ë²•        | ì„¤ëª…          | íŠ¹ì§•             |
| --------- | ----------- | -------------- |
| Zero      | 0ìœ¼ë¡œ ì´ˆê¸°í™”     | í•™ìŠµë˜ì§€ ì•ŠìŒ (ë¹„ê¶Œì¥)  |
| Random    | ë‚œìˆ˜ ì´ˆê¸°í™”      | ê³„ì¸µë§ˆë‹¤ ë¶ˆê· í˜• ê°€ëŠ¥    |
| He/Xavier | í™œì„±í™” í•¨ìˆ˜ì— ìµœì í™” | ReLU/íƒ„ì  íŠ¸ìš©ìœ¼ë¡œ ì¶”ì²œ |

---

## 2. í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ

* **ReLU**: ë¹ ë¥¸ ìˆ˜ë ´, ìŒìˆ˜ ì…ë ¥ì— ë¯¼ê°
* **Leaky ReLU**: ìŒìˆ˜ ì…ë ¥ ë³´ì™„
* **ELU/SELU**: ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ì—ì„œ í™œìš©

---

## 3. ì •ê·œí™” ê¸°ë²•

| ê¸°ë²•        | ì„¤ëª…           | íš¨ê³¼         |
| --------- | ------------ | ---------- |
| L1        | ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©   | í¬ì†Œí•œ ëª¨ë¸ ìƒì„±  |
| L2        | ê°€ì¤‘ì¹˜ ì œê³±í•©      | ì¼ë°˜ì  ê³¼ì í•© ë°©ì§€ |
| Dropout   | ì¼ë¶€ ë‰´ëŸ° ë¬´ì‘ìœ„ ì œê±° | ì•™ìƒë¸” íš¨ê³¼ ìœ ì‚¬  |
| BatchNorm | ë°°ì¹˜ ë‹¨ìœ„ ì •ê·œí™”    | í•™ìŠµ ì•ˆì •ì„± ì¦ê°€  |

---

## 4. Early Stopping

* ê²€ì¦ ì •í™•ë„ê°€ **í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ**
* `patience` ì„¤ì •ì„ í†µí•´ n epoch ë™ì•ˆ ê°œì„  ì—†ì„ ê²½ìš° ì¤‘ë‹¨

---

## 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

| ìš”ì†Œ            | ì„¤ëª…            |
| ------------- | ------------- |
| Learning Rate | ê°€ì¥ ë¯¼ê°í•œ íŒŒë¼ë¯¸í„°   |
| Batch Size    | ë©”ëª¨ë¦¬-ì†ë„ ê· í˜•     |
| Layer ìˆ˜       | ë³µì¡ë„ì™€ í•™ìŠµ ì‹œê°„ ì¡°ì • |
| Hidden Unit ìˆ˜ | ê³¼ì†Œ/ê³¼ì í•© ì¡°ì ˆ ë„êµ¬  |
| Activation    | ë¹„ì„ í˜•ì„± ì„ íƒ       |

---

## 6. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. Dropoutì´ ì ìš©ëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** í•™ìŠµ ì‹œ ì¼ë¶€ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ , ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ í†µí•´ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ì•™ìƒë¸” íš¨ê³¼ê°€ ë°œìƒí•©ë‹ˆë‹¤.

---

### ë¬¸ì œ 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ ëª¨ë¸ì˜ í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì„ ê²½ìš° ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ë‚˜ìš”?

ğŸŸ© **ì •ë‹µ:** ìˆ˜ë ´ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì ¸ í•™ìŠµ ì‹œê°„ì´ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ê¸¸ì–´ì§

---

## 7. ì‹¤ìŠµ: Early Stoppingê³¼ Dropoutì„ ì´ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
y = df['Machine failure']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ëª¨ë¸ ìƒì„±
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early Stopping ì½œë°±
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                    validation_split=0.2, callbacks=[early_stop], verbose=1)

# ì„±ëŠ¥ í‰ê°€
y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss with EarlyStopping")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **4ì¥. DNNì„ í™œìš©í•œ IoT ë°ì´í„° ë¶„ë¥˜**


## 1. ë¶„ë¥˜(Classification) ë¬¸ì œ ì •ì˜

* ì…ë ¥: ë‹¤ì–‘í•œ ì„¼ì„œ íŠ¹ì„± (ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, í•´ì¶© ì‚¬ì§„ ë“±)
* ì¶œë ¥: ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€ (ì˜ˆ: ê¸°ê³„ ê³ ì¥ ì—¬ë¶€, í•´ì¶© ì¢…ë¥˜)

---

## 2. ë¶„ë¥˜ ëª¨ë¸ì—ì„œì˜ DNN êµ¬ì„±


| í•­ëª©         | ì´ì§„ ë¶„ë¥˜               | ë‹¤ì¤‘ ë¶„ë¥˜ (one-hot encoding)               |
| ---------- | ------------------- | -------------------------- |
| ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜   | 1                   | Nê°œ (í•´ì¶© ì¢…ë¥˜ ìˆ˜)               |
| ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ | Sigmoid             | Softmax         |
| ì†ì‹¤ í•¨ìˆ˜      | Binary Crossentropy | Categorical Crossentropy (one-hot encoding) |
| ì˜ˆì¸¡ ë°©ì‹      | 0 or 1    (0~1 ì‚¬ì´ í™•ë¥ )  | ê° ë¼ë²¨ë³„ë¡œ 0 ë˜ëŠ” 1              |
| ì˜ˆì‹œ         | ê¸°ê³„ ê³ ì¥ ì—¬ë¶€            | í•´ì¶© ì¢…ë¥˜    |

---

## 3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

| ì§€í‘œ               | ì •ì˜                 | íŠ¹ì§•                   |
| ---------------- | ------------------ | -------------------- |
| Accuracy         | ì •í™•ë„ = (TP+TN)/(ì „ì²´) | í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œ ì™œê³¡ ê°€ëŠ¥      |
| Precision        | ì •ë°€ë„ = TP / (TP+FP) | ì˜ˆì¸¡ì´ ë§ì€ ë¹„ìœ¨            |
| Recall           | ì¬í˜„ìœ¨ = TP / (TP+FN) | ì‹¤ì œ ì–‘ì„± ì¤‘ ë§ì¶˜ ë¹„ìœ¨        |
| F1-score         | ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ ì¡°í™” í‰ê·      | ë¶ˆê· í˜• í´ë˜ìŠ¤ì— ì í•©          |
| Confusion Matrix | ì˜ˆì¸¡ vs ì‹¤ì œ ë¶„í¬í‘œ       | TP, TN, FP, FN êµ¬ì¡° í™•ì¸ |

- í˜¼ë™ í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„í•©ë‹ˆë‹¤:

| í•­ëª© | ì˜ë¯¸ |
|------|------|
| **True 1, Pred 1** | ì˜¬ë°”ë¥´ê²Œ ê³ ì¥ ì˜ˆì¸¡ (True Positive) |
| **True 0, Pred 0** | ì •ìƒ ì¥ë¹„ ì •í™• ì˜ˆì¸¡ (True Negative) |
| **True 0, Pred 1** | ì •ìƒì¸ë° ê³ ì¥ìœ¼ë¡œ ì˜ˆì¸¡ (False Positive) |
| **True 1, Pred 0** | ê³ ì¥ì¸ë° ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡ (False Negative) |
---

## 4. ì‹¤ì „ ì ìš©ì„ ìœ„í•œ ê³ ë ¤ ì‚¬í•­

* ì…ë ¥ ë³€ìˆ˜ì˜ **ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜**
* í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•œ ê²½ìš° **ê°€ì¤‘ì¹˜ ì¡°ì • ë˜ëŠ” ìƒ˜í”Œë§ ê¸°ë²•** ê³ ë ¤
* ì‹¤ì‹œê°„ IoT ì ìš©ì„ ìœ„í•œ **ëª¨ë¸ ê²½ëŸ‰í™”** ë° **ë°°í¬ ì „ëµ** í•„ìš”

---

## 5. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ ì ì ˆí•œ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** Sigmoid
ğŸ“ **ì„¤ëª…:** SigmoidëŠ” 0\~1 ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ë©°, í™•ë¥ ë¡œ í•´ì„í•  ìˆ˜ ìˆì–´ ì´ì§„ ë¶„ë¥˜ì— ì í•©í•©ë‹ˆë‹¤.

---
## 6. ì‹¤ìŠµ: ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ

### (1) ë¬¸ì œ
* **ìŠ¤ë§ˆíŠ¸ ì‹œí‹°ì˜ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ íŠ¹ì„± ê¸°ë°˜ ì¹¨ì… íƒì§€ ë° ê³µê²© ìœ í˜• ë¶„ë¥˜**í•˜ëŠ” ë¬¸ì œ.
* ë°ì´í„° ì…‹: ê³µê°œ ë°ì´í„°ì¸ [UNSW-NB15 ë„¤íŠ¸ì›Œí¬ ì¹¨ì… íƒì§€ ë°ì´í„°ì…‹](https://figshare.com/articles/dataset/UNSW_NB15_training-set_csv/29149946?file=54850502)
- ì‚¬ì´íŠ¸ì—ì„œ **UNSW_NB15_training-set.csv** íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ ,  Colab í™˜ê²½ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.


#### ğŸ“˜ `UNSW_NB15_training-set.csv` íŒŒì¼ì˜ ì£¼ìš” ì»¬ëŸ¼ë“¤


| ì»¬ëŸ¼ ì´ë¦„     | ì„¤ëª…                                 |
| --------- | ---------------------------------- |
| `id`      | ê° í–‰ì˜ ê³ ìœ  ì‹ë³„ì                        |
| `dur`     | ì„¸ì…˜ ì§€ì† ì‹œê°„ (ì´ˆ ë‹¨ìœ„)                    |
| `proto`   | ì‚¬ìš©ëœ ì „ì†¡ í”„ë¡œí† ì½œ (ì˜ˆ: TCP, UDP, ICMP ë“±)  |
| `service` | ëª©ì ì§€ ì„œë¹„ìŠ¤ ìœ í˜• (ì˜ˆ: HTTP, FTP, SMTP ë“±)  |
| `state`   | íŠ¸ë˜í”½ ì„¸ì…˜ì˜ ìƒíƒœ ì½”ë“œ (ì˜ˆ: FIN, REJ, INT ë“±) |
| `spkts`   | ì†ŒìŠ¤ì—ì„œ ë³´ë‚¸ íŒ¨í‚· ìˆ˜                       |
| `dpkts`   | ëª©ì ì§€ì—ì„œ ë³´ë‚¸ íŒ¨í‚· ìˆ˜                      |
| `sbytes`  | ì†ŒìŠ¤ê°€ ì „ì†¡í•œ ë°”ì´íŠ¸ ìˆ˜                      |
| `dbytes`  | ëª©ì ì§€ê°€ ì „ì†¡í•œ ë°”ì´íŠ¸ ìˆ˜                     |
| `rate`    | í‰ê·  íŒ¨í‚· ì „ì†¡ë¥                           |
| `sload`, `dload`   | ì†ŒìŠ¤/ëª©ì ì§€ì˜ ë°ì´í„° ì „ì†¡ ì†ë„ (bps)                |
| `stcpb`, `dtcpb`   | TCP ì´ˆê¸° ì‹œí€€ìŠ¤ ë²ˆí˜¸                          |
| `tcprtt`           | TCP ì™•ë³µ ì‹œê°„ (RTT)                        |
| `synack`, `ackdat` | TCP í•¸ë“œì…°ì´í¬ ê´€ë ¨ ì‹œê°„ ì •ë³´                     |
| `is_sm_ips_ports`  | ë™ì¼í•œ IP/í¬íŠ¸ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ì—¬ë¶€                    |
| `attack_cat`       | ê³µê²© ìœ í˜• ë¶„ë¥˜ (ì˜ˆ: Fuzzers, Exploits, DoS ë“±) |
| `label`            | ì´ì§„ ë ˆì´ë¸” (0 = ì •ìƒ, 1 = ê³µê²©)                |


* `attack_cat` ì»¬ëŸ¼ì€ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ í™œìš© ê°€ëŠ¥
* `label` ì»¬ëŸ¼ì€ ì´ì§„ ë¶„ë¥˜ìš© ë ˆì´ë¸”
* ëŒ€ë¶€ë¶„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ **ì •ê·œí™” ë˜ëŠ” ìŠ¤ì¼€ì¼ë§ í•„ìš”**
* `proto`, `service`, `state` ë“±ì€ **ë²”ì£¼í˜• ì¸ì½”ë”© í•„ìš”**


#### ğŸ“˜ ê³µê²© ìœ í˜•(`attack_cat` ì»¬ëŸ¼): 10ì¢…ë¥˜


| ì¸ë±ìŠ¤ | ê³µê²© ìœ í˜• (`attack_cat`)        | ì„¤ëª…                                                                  |
| --- | --------------------------- | ------------------------------------------------------------------- |
| 0   | **Analysis**                | í¬íŠ¸ ìŠ¤ìº”, íŒ¨í‚· ìº¡ì²˜, IDS íšŒí”¼ ë“± ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ë¶„ì„í•˜ê¸° ìœ„í•œ í™œë™. ì¢…ì¢… ì •ì°°ì´ë‚˜ ì¹¨íˆ¬ ì¤€ë¹„ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.         |
| 1   | **Backdoor**                | ì‹œìŠ¤í…œì— ëª°ë˜ ì ‘ê·¼í•˜ëŠ” ê²½ë¡œë¥¼ ë§Œë“œëŠ” ê³µê²© (ex: ì›ê²© ì…¸ ìƒì„±). ì‹œìŠ¤í…œì— ë¹„ì •ìƒì ìœ¼ë¡œ ì ‘ê·¼í•˜ê¸° ìœ„í•´ ì„¤ì¹˜ëœ ë¹„ë°€ ê²½ë¡œë¥¼ í†µí•´ ì¸ì¦ ì—†ì´ ì ‘ê·¼ ê°€ëŠ¥í•œ ê³µê²©ì…ë‹ˆë‹¤.               |
| 2   | **DoS (Denial of Service)** | ì„œë¹„ìŠ¤ ê±°ë¶€ ê³µê²©. ê³¼ë„í•œ íŠ¸ë˜í”½ìœ¼ë¡œ ìì›ì„ ì†Œëª¨ì‹œì¼œ ì •ìƒ ì„œë¹„ìŠ¤ ë°©í•´. ì„œë¹„ìŠ¤ ê±°ë¶€ ê³µê²©ìœ¼ë¡œ, ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ì‹œìŠ¤í…œì„ ë§ˆë¹„ì‹œì¼œ ì •ìƒ ì‚¬ìš©ìê°€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì§€ ëª»í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.         |
| 3   | **Exploits**                | ë³´ì•ˆ ì·¨ì•½ì ì„ ì´ìš©í•˜ì—¬ ì‹œìŠ¤í…œ ì œì–´ê¶Œì„ íšë“í•˜ëŠ” ê³µê²©. ì†Œí”„íŠ¸ì›¨ì–´, í•˜ë“œì›¨ì–´, ë˜ëŠ” OSì˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ì•…ìš©í•˜ì—¬ ê¶Œí•œ ìƒìŠ¹ ë˜ëŠ” ëª…ë ¹ ì‹¤í–‰ ë“±ì„ ìœ ë„í•˜ëŠ” ê³µê²©ì…ë‹ˆë‹¤.      |
| 4   | **Fuzzers**                 | ì‹œìŠ¤í…œ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ì— ë¹„ì •ìƒì ì¸ ì…ë ¥ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì‚½ì…í•˜ì—¬ ì¶©ëŒì´ë‚˜ ì·¨ì•½ì ì„ ìœ ë°œí•˜ëŠ” ê³µê²©. ë‹¤ì–‘í•œ ì…ë ¥ ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì „ì†¡í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì·¨ì•½ì ì„ ë°œê²¬í•˜ë ¤ëŠ” ê¸°ë²•. ì¢…ì¢… ì‹œìŠ¤í…œ ì¶©ëŒì´ë‚˜ ì˜ˆì™¸ ë°œìƒì„ ìœ ë„í•©ë‹ˆë‹¤.   |
| 5   | **Generic**                 | ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ì  ì•½ì ì„ í™œìš©í•˜ì—¬ ê³µê²©í•˜ëŠ” ë°©ì‹. ì˜ˆë¥¼ ë“¤ì–´, ë™ì¼í•œ í‚¤ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œì„ ë…¸ë¦½ë‹ˆë‹¤. |
| 6   | **Normal**                  | ì •ìƒì ì¸ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ìœ¼ë¡œ, ê³µê²©ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì‚¬ìš©ì í–‰ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.                         |
| 7   | **Reconnaissance**          | í¬íŠ¸ ìŠ¤ìºë‹, ë„¤íŠ¸ì›Œí¬ ë§¤í•‘ ë“±ê³¼ ê°™ì€ ì •ë³´ ìˆ˜ì§‘ í–‰ìœ„. ê³µê²©ì„ ìœ„í•œ ì‚¬ì „ ì¡°ì‚¬ì— í•´ë‹¹í•©ë‹ˆë‹¤.                |
| 8   | **Shellcode**               | ì‰˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë°”ì´ë„ˆë¦¬ ì½”ë“œë¡œ, ì£¼ë¡œ ì·¨ì•½ì ì„ ì•…ìš©í•´ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.             |
| 9   | **Worms**                   | ìì²´ ë³µì œ ë° í™•ì‚° ê¸°ëŠ¥ì„ ê°–ì¶˜ ì•…ì„±ì½”ë“œ (ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ê°ì—¼). ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ë³µì œ ë° í™•ì‚°ë˜ë©° ë‹¤ë¥¸ ì‹œìŠ¤í…œì„ ê°ì—¼ì‹œí‚¤ëŠ” ì•…ì„± í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.                    |



* `attack_cat` ì»¬ëŸ¼ì€ `LabelEncoder`ë¥¼ ì‚¬ìš©í•´ `0~9`ì˜ ì •ìˆ˜í˜• í´ë˜ìŠ¤ë¡œ ë³€í™˜ë¨.
* ì´í›„ `to_categorical()` ì²˜ë¦¬í•´ **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì˜ íƒ€ê²Ÿ**ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.
* `Normal`ê³¼ ë‚˜ë¨¸ì§€ ê³µê²©ë“¤ì„ êµ¬ë¶„í•˜ë©´ **ì´ì§„ ë¶„ë¥˜**, ì „ì²´ë¥¼ êµ¬ë¶„í•˜ë©´ **ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ**ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.

---

### (2) ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œì˜ ë”¥ëŸ¬ë‹ êµ¬ì¡°

* ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜ = í´ë˜ìŠ¤ ìˆ˜ (ì˜ˆ: 10 ê°€ì§€ì˜ ê³µê²© ìœ í˜• â†’ 10ê°œ ë…¸ë“œ)
* ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜: `softmax`
* ì†ì‹¤ í•¨ìˆ˜: `categorical_crossentropy`

---
### (3) ì‹¤ìŠµ ì½”ë“œ

#### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„° ë¡œë“œ

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# CSV ë¡œë“œ
df = pd.read_csv('/content/UNSW_NB15_training-set.csv')
df.head()

df.info()

df.nunique()

df.isnull().sum()

df.isin([np.inf, -np.inf]).sum()
```

---

#### 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬

```python

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬: idì™€  label ë¶ˆí•„ìš”
X = df.drop(columns=['id', 'attack_cat', 'label'])
y = df['attack_cat']

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
#X.replace([np.inf, -np.inf], np.nan, inplace=True)
#X.fillna(0, inplace=True)

# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# ë ˆì´ë¸” ì¸ì½”ë”© 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
class_labels = label_encoder.classes_
print(class_labels)

#  ì›-í•« ì¸ì½”ë”©
y_onehot = to_categorical(y_encoded)

```

---

#### 3. í›ˆë ¨Â·ê²€ì¦ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§

```python

# ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
X_train, X_test, y_train, y_test = train_test_split(
		X, y_onehot, test_size=0.2, stratify=y_encoded)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


```

---

#### 4. MLP ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•

```python

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(y_onehot.shape[1], activation='softmax')
])

model.compile(optimizer='adam', 
		loss='categorical_crossentropy', 
		metrics=['accuracy'])
```

---

#### 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

```python


## EarlyStopping + ModelCheckpoint
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint("unsw_nb15_model.h5", save_best_only=True, monitor='val_loss')

# í•™ìŠµ
history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=64,
                    validation_split=0.2, 
		    callbacks=[early_stop, checkpoint], verbose=1)
# ì˜ˆì¸¡ ë° í‰ê°€
y_pred_proba = model.predict(X_test_scaled)
y_pred = np.argmax(y_pred_proba, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred, target_names=class_labels))

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
```

---

#### 6. í•™ìŠµ ê³¡ì„  ì‹œê°í™”

```python
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy over Epochs")
plt.legend()
plt.grid(True)
plt.show()
```

----
## 7. ë‹¤ì¤‘ë¶„ë¥˜ ê°™ì§€ë§Œ ë‹¤ì¤‘ë¶„ë¥˜ê°€ ì•„ë‹Œ ë©€í‹° ë¼ë²¨ ë¶„ë¥˜ ë¬¸ì œ

Ai4I 2020 Predictive Maintenance Datasetì—ì„œ ê³ ì¥ì˜ **ì›ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ** 


> ë‹¤ì¤‘ë¶„ë¥˜ì˜ ê²½ìš° ì¶œë ¥ì¸µì— ìˆëŠ” ë‰´ëŸ° ì¤‘ ì˜¤ì§ í•œ ê°œì˜ ë‰´ëŸ°ë§Œ ì¶œë ¥ '1'ì´ ë‚˜ì˜µë‹ˆë‹¤.
> ê·¸ëŸ¬ë‚˜, ê¸°ê³„ ê³ ì¥ ë¬¸ì œëŠ” ì´ ë¬¸ì œëŠ” **Multi-label classification**ì…ë‹ˆë‹¤.
> ì¦‰, í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ê³ ì¥ ìœ í˜•ì´ ë™ì‹œì— ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œê°€ 1ì„ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” **ë‹¤ì¤‘ ê³ ì¥ ë™ì‹œ ë°œìƒ ê°€ëŠ¥ì„±**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> ì´ëŸ° ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ êµ¬ì„± ì½”ë“œê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
---

### (1) ë©€í‹° ë¼ë²¨ ë¶„ë¥˜ì—ì„œì˜ ë”¥ëŸ¬ë‹ êµ¬ì¡°

* ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜ = í´ë˜ìŠ¤ ìˆ˜ (ì˜ˆ: 5ê°œ ê³ ì¥ ì›ì¸ â†’ 5ê°œ ë…¸ë“œ)
* ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜: `sigmoid`
* ì†ì‹¤ í•¨ìˆ˜: `binary_crossentropy`


| ê³ ì¥ ìœ í˜• | ì˜ˆì¸¡ ê°’ (ì¶œë ¥ ë…¸ë“œ) | ì˜ë¯¸              |
| ----- | ------------ | --------------- |
| TWF   | 1            | ê³µêµ¬ ë§ˆëª¨ë¡œ ì¸í•œ ê³ ì¥ ë°œìƒ |
| HDF   | 0            | íˆí„° ê³ ì¥ ì—†ìŒ        |
| ...   | ...          | ...             |

* í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œê°€ 1ì„ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” **ë‹¤ì¤‘ ê³ ì¥ ë™ì‹œ ë°œìƒ ê°€ëŠ¥ì„±**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

---

### (2) ì‹¤ìŠµ ì½”ë“œ: ë‹¤ì¤‘ ê³ ì¥ ì›ì¸ ë¶„ë¥˜

| ë‹¨ê³„         | ë‚´ìš©                                    |
| ---------- | ------------------------------------- |
| **ì…ë ¥ ë°ì´í„°** | Type, ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, ë§ˆëª¨ ì‹œê°„            |
| **ì¶œë ¥ (y)** | ë‹¤ì¤‘ ì´ì§„ ê³ ì¥ ìœ í˜• (TWF, HDF, PWF, OSF, RNF) |
| **ëª¨ë¸ êµ¬ì¡°**  | 128 â†’ 64 â†’ 5 (sigmoid)                |
| **ì†ì‹¤ í•¨ìˆ˜**  | `binary_crossentropy` (ë‹¤ì¤‘ ì´ì§„ íƒ€ê²Ÿìš©)     |
| **í™œì„±í™” í•¨ìˆ˜** | ReLU + sigmoid ì¶œë ¥                     |
| **ì˜ˆì¸¡ í•´ì„**  | ê° ê³ ì¥ ìœ í˜•ë³„ 0.5 ì„ê³„ê°’ ì ìš©                   |
| **ì„±ëŠ¥ í‰ê°€**  | ê° ê³ ì¥ ìœ í˜•ë³„ `classification_report` ì¶œë ¥   |



```python
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam


# 2. ë°ì´í„° ë¡œë“œ
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)

# 3. ë°ì´í„° ì „ì²˜ë¦¬
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
 # ê³ ì¥ ì •ë³´ê°€ ì—†ëŠ” í–‰(ëª¨ë“  ì—´ì´ 0) ì„ ì‚­ì œ
df = df[df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].sum(axis=1) > 0]
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
X.columns = ['Type', 'AirTemp', 'ProcTemp', 'RotSpeed', 'Torque', 'ToolWear']

y = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']]

# 4. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

y

y.sum()

y_test

y_test.sum()


```

```python
# 5. ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
    Dense(64, activation='relu'),
        Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(5, activation='sigmoid')
])


model.compile(optimizer=Adam(learning_rate = 0.002), 
              loss='binary_crossentropy', metrics=['accuracy'])

# 6. ì½œë°± ì„¤ì • (EarlyStopping & ModelCheckpoint)
early_stop = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True, verbose=1)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)

# 7. í•™ìŠµ
history = model.fit(X_train, y_train,
                    epochs=150,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stop, checkpoint],
                    verbose=1)

# 8. ëª¨ë¸ ì˜ˆì¸¡
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# 9. ê° í´ë˜ìŠ¤ë³„ í‰ê°€ ë° í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
for i, col in enumerate(y.columns):
    print(f"\n[ê³ ì¥ ìœ í˜•: {col}]")
    print(classification_report(y_test[col], y_pred[:, i]))

    cm = confusion_matrix(y_test[col], y_pred[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Pred 0', 'Pred 1'],
                yticklabels=['True 0', 'True 1'])
    plt.title(f'Confusion Matrix: {col}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.tight_layout()
    plt.show()

# 10. í•™ìŠµ ì •í™•ë„ ì‹œê°í™”
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Multi-label Classification Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

```

---

# ğŸ“˜ **5ì¥. DNNì„ í™œìš©í•œ íšŒê·€**

## 1. íšŒê·€ ë¬¸ì œì˜ ì •ì˜

* ì…ë ¥: ì„¼ì„œ íŠ¹ì„± (ì˜¨ë„, í† í¬, íšŒì „ìˆ˜ ë“±)
* ì¶œë ¥: **ì—°ì†ê°’** (ì˜ˆ: ê³ ì¥ ì‹œì ê¹Œì§€ ë‚¨ì€ ì‹œê°„, ë§ˆëª¨ ì‹œê°„ ë“±)

---

## 2. ë¶„ë¥˜ì™€ íšŒê·€ì˜ ì£¼ìš” ì°¨ì´

| í•­ëª©         | ë¶„ë¥˜                       | íšŒê·€                    |
| ---------- | ------------------------ | --------------------- |
| ì¶œë ¥ê°’        | ë²”ì£¼ (0, 1, ë‹¤ì¤‘ í´ë˜ìŠ¤)        | ì—°ì†í˜• ìˆ˜ì¹˜                |
| ì¶œë ¥ì¸µ ë…¸ë“œ     | 1ê°œ ì´ìƒ (sigmoid, softmax) | 1ê°œ (ì„ í˜•)               |
| ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ | sigmoid/softmax          | **None (Linear)**     |
| ì†ì‹¤ í•¨ìˆ˜      | crossentropy             | **MSE / MAE / Huber** |

---

## 3. íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

| ì§€í‘œ                        | ìˆ˜ì‹                                 | í•´ì„                   |   |               |
| ------------------------- | ---------------------------------- | -------------------- | - | ------------- |
| MSE (Mean Squared Error)  | $\frac{1}{n} \sum (y - \hat{y})^2$ | ì˜¤ì°¨ ì œê³± í‰ê·  (ë¯¼ê°)        |   |               |
| MAE (Mean Absolute Error) | ( \frac{1}{n} \sum                 | y - \hat{y}          | ) | ì ˆëŒ€ ì˜¤ì°¨ í‰ê·  (ê°•ê±´) |
| RMSE                      | $\sqrt{MSE}$                       | í•´ì„ ìš©ì´                |   |               |
| RÂ² Score                  | $1 - \frac{SS_{res}}{SS_{tot}}$    | ì„¤ëª…ë ¥ ì§€í‘œ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ) |   |               |

---

## 4. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. íšŒê·€ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ ì ì ˆí•œ ê²ƒì€?

ğŸŸ© **ì •ë‹µ:** ì—†ìŒ (ì„ í˜•)
ğŸ“ **ì„¤ëª…:** íšŒê·€ì—ì„œëŠ” ì—°ì†ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ë¯€ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

---

### ë¬¸ì œ 2. MSEì™€ MAEì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** MSEëŠ” ì˜¤ì°¨ ì œê³±ì„ ì‚¬ìš©í•˜ì—¬ í° ì˜¤ì°¨ì— ë¯¼ê°í•˜ê³ , MAEëŠ” ì ˆëŒ€ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì— ê°•ê±´í•©ë‹ˆë‹¤.

---

### ë¬¸ì œ 3. ë‹¤ìŒ ì¤‘ RÂ² Scoreê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”?

ğŸŸ© **ì •ë‹µ:** ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ê³  ì˜ˆì¸¡ë ¥ì´ ë†’ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

---

## 5. ì‹¤ìŠµ: **DNN íšŒê·€ ê¸°ë°˜ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡**

### (1) ì£¼ìš” ë³€ê²½

| í•­ëª© | ì„ í˜• íšŒê·€ | MLP íšŒê·€ |
|------|------------|------------|
| ëª¨ë¸ | `LinearRegression()` | `Keras Sequential` |
| ì†ì‹¤ í•¨ìˆ˜ | MSE | MSE |
| ì¶œë ¥ì¸µ | ì„ í˜• | ì„ í˜• (`Dense(1)`) |
| í‰ê°€ | RMSE, RÂ² | ë™ì¼ |

### (2) ì ìš© ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì ìš© ë‚´ìš© |
|------|-----------|
| **ì •ê·œí™”** | `kernel_regularizer=l2(0.001)` |
| **Dropout** | ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ 30%, ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ 20% |
| **EarlyStopping** | `patience=10`, `restore_best_weights=True` |

### (3) ì‹¤ìŠµ ì½”ë“œ

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
climate_path = "/content/GreenhouseClimate.csv"
prod_path = "/content/Production.csv"

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
climate = pd.read_csv(climate_path)
climate['Time'] = pd.to_datetime(climate['Time'], unit='D', origin='1900-01-01')

production = pd.read_csv(prod_path)
production['Time'] = pd.to_datetime(production['Time'], unit='D', origin='1900-01-01')

# 3. í•„ìš”í•œ ë³€ìˆ˜ë§Œ ì¶”ì¶œ
climate = climate[['Time', 'Tair', 'Rhair', 'CO2air', 'Tot_PAR', 'Cum_irr']]
production = production[['Time', 'ProdA']]  # ëª©í‘œ: Class A ìˆ˜í™•ëŸ‰

# 4. ì‹œê°„ ë‹¨ìœ„ í‰ê·  (í•˜ë£¨ ë‹¨ìœ„ë¡œ)
climate_indexed = climate.set_index('Time') 
production_indexed = production.set_index('Time')

numerical_cols = ['Tair', 'Rhair', 'CO2air', 'Tot_PAR', 'Cum_irr']
for col in numerical_cols:
    climate_indexed[col] = pd.to_numeric(climate_indexed[col], errors='coerce')

climate_daily = climate_indexed[numerical_cols].resample('D').mean().reset_index()
production_daily = production_indexed.resample('D').sum().reset_index()

# 5. ë³‘í•© ë° ê²°ì¸¡ì¹˜ ì œê±°
df = pd.merge(climate_daily, production_daily, on='Time')
df.dropna(inplace=True)

# 6. X, y ë¶„ë¦¬
X = df[numerical_cols]
y = df['ProdA']

# 7. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

```python
# 9. ëª¨ë¸ êµ¬ì„± (L2 ì •ê·œí™” + Dropout í¬í•¨)
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],),
          kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.2),
    Dense(1)  # íšŒê·€ ë¬¸ì œ: ì„ í˜• ì¶œë ¥
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# 10. EarlyStopping ì½œë°± ì„¤ì •
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# 11. ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train_scaled, y_train,
                    epochs=100,
                    batch_size=16,
                    validation_split=0.2,
                    callbacks=[early_stop],
                    verbose=0)
```

```python

# 12. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test_scaled).flatten()
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f" RMSE (MLP): {rmse:.2f}")
print(f" RÂ² Score (MLP): {r2:.2f}")

# 13. ì˜ˆì¸¡ vs ì‹¤ì œê°’ ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(y_test.values, label='Ground Truth')
plt.plot(y_pred, label='Predicted (MLP)', linestyle='--')
plt.title("Tomato Production Prediction (MLP + Reg + Dropout)")
plt.xlabel("Sample Index")
plt.ylabel("Production (kg/mÂ²)")
plt.legend()
plt.grid()
plt.show()

# 14. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title("MAE History with EarlyStopping")
plt.xlabel("Epoch")
plt.ylabel("MAE")
plt.legend()
plt.grid()
plt.show()

```

---

=======
# - https://shorturl.at/aLUfI (ëŒ€ë¬¸ì i)

# - https://shorturl.at/3nQFr
--- 
# ğŸ“˜ **5ë¶€. ë”¥ëŸ¬ë‹ ê¸°ìˆ ê³¼ í™œìš©**
---

# ğŸ“˜ **1ì¥. ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ê°œìš”ì™€ êµ¬ì¡°**



ì‹¬ì¸µ ì‹ ê²½ë§(Deep Neural Network, DNN)ì€ ê¸°ì¡´ì˜ ì¸ê³µì‹ ê²½ë§(ANN, Artificial Neural Network)ì—ì„œ ë°œì „í•œ êµ¬ì¡°ë¡œ, 2006ë…„ Geoffrey Hintonê³¼ Yoshua Bengio ë“±ì´ ì œì•ˆí•œ **ë”¥ëŸ¬ë‹(Deep Learning)** ê°œë…ì´ ë„ë¦¬ í¼ì§€ë©´ì„œ ì£¼ëª©ë°›ê¸° ì‹œì‘í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ˆê¸°ì—ëŠ” XOR ë¬¸ì œ í•´ê²°ì´ë‚˜ ìŒì„± ì¸ì‹ ë“±ì— í™œìš©ë˜ì—ˆì§€ë§Œ, **2012ë…„ ILSVRCì—ì„œ AlexNetì´ ëŒ€íšŒ ìš°ìŠ¹**ì„ í•˜ë©´ì„œ ì´ë¯¸ì§€, ìŒì„±, ì‹œê³„ì—´ ë°ì´í„° ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ DNNì´ í‘œì¤€ìœ¼ë¡œ ìë¦¬ ì¡ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
- [ìœ„í‚¤í”¼ë””ì•„:ì‹¬ì¸µì‹ ê²½ë§](https://en.wikipedia.org/wiki/Neural_network_(machine_learning))
- [ìœ„í‚¤ë°±ê³¼: ë”¥ëŸ¬ë‹](https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D)
- [ìœ„í‚¤ë…ìŠ¤: deep neural network](https://wikidocs.net/120152)
---


## 1. ì¸ê³µì‹ ê²½ë§(ANN)ì˜ êµ¬ì¡° 

| ê³„ì¸µ ìœ í˜•             | ì„¤ëª…         |
| ----------------- | ---------- |
| ì…ë ¥ì¸µ(Input Layer)  | ë°ì´í„° ì…ë ¥ì„ ë‹´ë‹¹ |
| ì€ë‹‰ì¸µ(Hidden Layer) | ë‚´ë¶€ íŠ¹ì§• ì¶”ì¶œ   |
| ì¶œë ¥ì¸µ(Output Layer) | ìµœì¢… ì˜ˆì¸¡ê°’ ìƒì„±  |

## 2. ì‹¬ì¸µ ì‹ ê²½ë§(DNN)ì´ë€?

* ì€ë‹‰ì¸µì´ **2ê°œ ì´ìƒ**ì¸ ANN êµ¬ì¡°ë¥¼ DNNì´ë¼ í•©ë‹ˆë‹¤.
* ê° ê³„ì¸µì€ **ê°€ì¤‘ì¹˜(weight)**, **í¸í–¥(bias)**, \*\*í™œì„±í™” í•¨ìˆ˜(activation)\*\*ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

## 3. ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ ë¹„êµ

| í•¨ìˆ˜      | ìˆ˜ì‹                                  | íŠ¹ì§•                    |
| ------- | ----------------------------------- | --------------------- |
| ReLU    | $f(x) = \max(0, x)$                 | ë¹ ë¥¸ í•™ìŠµ, ìŒìˆ˜ì—ì„œ ì£½ëŠ” ë‰´ëŸ° ê°€ëŠ¥ì„± |
| Sigmoid | $\frac{1}{1 + e^{-x}}$              | ì¶œë ¥ 0\~1, ê²½ì‚¬ ì†Œì‹¤ ë¬¸ì œ ì¡´ì¬  |
| Tanh    | $\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | -1\~1 ì¶œë ¥, ì¤‘ì‹¬í™”ëœ ì¶œë ¥     |

---

## 4. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multilayer Perceptron, MLP)ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ 3ê°€ì§€ë¥¼ ê¸°ìˆ í•˜ì‹œì˜¤.

ğŸŸ© **ì •ë‹µ:** ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µ

ğŸ“ **ì„¤ëª…:** MLPëŠ” ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ì€ë‹‰ì¸µê³¼ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ í¬í•¨í•´ì•¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ë¬¸ì œ 2. ReLU í•¨ìˆ˜ì˜ ì£¼ìš” ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** ìŒìˆ˜ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ì´ 0ì´ ë˜ì–´ í•™ìŠµì´ ë©ˆì¶œ ìˆ˜ ìˆìŒ (ì£½ì€ ë‰´ëŸ° ë¬¸ì œ)

ğŸ“ **ì„¤ëª…:** ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Leaky ReLU, ELU ë“±ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

---

## 5. ì‹¤ìŠµ: Ai4I 2020 Datasetì„ í™œìš©í•œ MLP ë¶„ë¥˜


| ë‹¨ê³„         | ë‚´ìš©                                    |
| ---------- | ------------------------------------- |
| **ì…ë ¥ ë°ì´í„°** | Type, ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, ë§ˆëª¨ ì‹œê°„            |
| **ì¶œë ¥ (y)** | ê³ ì¥(1), ì •ìƒ (0)|
| **ëª¨ë¸ êµ¬ì¡°**  | 64 â†’ 32 â†’ 1 (sigmoid)                |
| **ì†ì‹¤ í•¨ìˆ˜**  | `binary_crossentropy` (ë‹¤ì¤‘ ì´ì§„ íƒ€ê²Ÿìš©)     |



```python
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

# 3. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ì¶”ì¶œ
X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
y = df['Machine failure']

# 4. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. MLP ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. í•™ìŠµ ìˆ˜í–‰
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# 7. í‰ê°€
y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

# 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title("Training History")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **2ì¥. í•™ìŠµê³¼ ìµœì í™” ì•Œê³ ë¦¬ì¦˜**



ì´ˆê¸° ì‹ ê²½ë§ í•™ìŠµ ë°©ì‹ì€ ë‹¨ìˆœí•œ \*\*ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)\*\*ì´ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸µì´ ê¹Šì–´ì§€ë©´ì„œ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” **ê¸°ìš¸ê¸° ì†Œì‹¤(vanishing gradient)** ë¬¸ì œê°€ ë°œìƒí–ˆê³ , 1986ë…„ Rumelhart, Hinton, Williamsê°€ ì œì•ˆí•œ **ì—­ì „íŒŒ(Backpropagation)** ì•Œê³ ë¦¬ì¦˜ì´ ì´ë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.
ì´í›„ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ í–¥ìƒì„ ìœ„í•´ ë‹¤ì–‘í•œ \*\*ìµœì í™” ì•Œê³ ë¦¬ì¦˜(SGD, Momentum, RMSProp, Adam ë“±)\*\*ì´ ì œì•ˆë˜ë©°, ë”¥ëŸ¬ë‹ì˜ í­ë°œì  ë°œì „ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.

---


## 1. ìˆœì „íŒŒ(Forward Propagation)

* ì…ë ¥ ë°ì´í„°ë¥¼ ê³„ì¸µì„ ë”°ë¼ \*\*ì„ í˜• ë³€í™˜(Wx + b)\*\*ê³¼ **ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜**ë¥¼ í†µí•´ ì¶œë ¥ê¹Œì§€ ì „ë‹¬í•©ë‹ˆë‹¤.
* ì˜ˆì‹œ:

  $$
  z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)} \quad,\quad a^{(l)} = \text{ReLU}(z^{(l)})
  $$

---

## 2. ì†ì‹¤ í•¨ìˆ˜(Loss Function)

* **ë¶„ë¥˜:** Binary Cross Entropy

  $$
  \mathcal{L} = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
  $$
* **íšŒê·€:** MSE, MAE ì‚¬ìš©

---

## 3. ì—­ì „íŒŒ(Backpropagation)

* ì†ì‹¤ í•¨ìˆ˜ì˜ ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ **ì²´ì¸ë£°ë¡œ ë¯¸ë¶„**í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
* ê° ê³„ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë‹¤ìŒ ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:

  $$
  W := W - \eta \cdot \frac{\partial \mathcal{L}}{\partial W}
  $$

  $$
  b := b - \eta \cdot \frac{\partial \mathcal{L}}{\partial b}
  $$

---

## 4. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜     | ì„¤ëª…                 | íŠ¹ì§•        |
| -------- | ------------------ | --------- |
| SGD      | í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•          | ë¶ˆì•ˆì •í•˜ì§€ë§Œ ë¹ ë¦„ |
| Momentum | ì†ë„ ê°œë… ë„ì…           | ì§„ë™ ì–µì œ     |
| RMSProp  | ìµœê·¼ ê¸°ìš¸ê¸° ì œê³± í‰ê·        | í•™ìŠµë¥  ìë™ ì¡°ì ˆ |
| Adam     | Momentum + RMSProp | ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨ |

---

## 5. í•™ìŠµ ê´€ë ¨ ìš©ì–´

| ìš©ì–´            | ì •ì˜                     |
| ------------- | ---------------------- |
| Batch         | í•œ ë²ˆ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œ ìˆ˜      |
| Epoch         | ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ ëª¨ë‘ í•™ìŠµ      |
| Learning Rate | ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ í­             |
| ì •ê·œí™”           | ê³¼ì í•© ë°©ì§€ (L2, Dropout ë“±) |

---

## 6. ì‹¤ìŠµ: Optimizerì— ë”°ë¥¸ í•™ìŠµ ì„±ëŠ¥ ë¹„êµ (Ai4I 2020 Dataset)

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam, RMSprop
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ë™ì¼í•˜ë¯€ë¡œ ì•„ë˜ì„œë¶€í„° ìˆ˜í–‰

# ëª¨ë¸ ìƒì„± í•¨ìˆ˜
def create_model(optimizer):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# ìµœì í™”ê¸° ë³„ í•™ìŠµ ê²°ê³¼ ì €ì¥
optimizers = {'SGD': SGD(), 'RMSprop': RMSprop(), 'Adam': Adam()}
histories = {}

for name, opt in optimizers.items():
    print(f"\n[ {name} Optimizer í•™ìŠµ ì‹œì‘ ]")
    model = create_model(opt)
    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0)
    histories[name] = history
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    print(f"{name} Optimizer ì„±ëŠ¥:\n", classification_report(y_test, y_pred))

# ì •í™•ë„ ì‹œê°í™”
plt.figure(figsize=(10, 5))
for name, history in histories.items():
    plt.plot(history.history['val_accuracy'], label=f'{name} Val Acc')
plt.title("Optimizerë³„ Validation Accuracy ë¹„êµ")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **3ì¥. ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì „ëµ**


ì´ˆê¸° ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ **í•™ìŠµì´ ëŠë¦¬ê³  ë¶ˆì•ˆì •**í•˜ë©°, **ê³¼ì í•©(overfitting)** ë¬¸ì œê°€ ìì£¼ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2010ë…„ëŒ€ ì´ˆë°˜ ë‹¤ì–‘í•œ ê¸°ìˆ ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.

* **Dropout**(2014, Hinton)ì€ ì€ë‹‰ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•´ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ê³ ,
* **Batch Normalization**(2015, Ioffe & Szegedy)ì€ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì†ë„ë¥¼ í¬ê²Œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
* ë˜í•œ, **Early Stopping**, **Regularization**, **Hyperparameter Tuning** ë“±ë„ ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ì „ëµìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.

---


## 1. ì´ˆê¸°í™” ì „ëµ (Weight Initialization)

| ë°©ë²•        | ì„¤ëª…          | íŠ¹ì§•             |
| --------- | ----------- | -------------- |
| Zero      | 0ìœ¼ë¡œ ì´ˆê¸°í™”     | í•™ìŠµë˜ì§€ ì•ŠìŒ (ë¹„ê¶Œì¥)  |
| Random    | ë‚œìˆ˜ ì´ˆê¸°í™”      | ê³„ì¸µë§ˆë‹¤ ë¶ˆê· í˜• ê°€ëŠ¥    |
| He/Xavier | í™œì„±í™” í•¨ìˆ˜ì— ìµœì í™” | ReLU/íƒ„ì  íŠ¸ìš©ìœ¼ë¡œ ì¶”ì²œ |

---

## 2. í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ

* **ReLU**: ë¹ ë¥¸ ìˆ˜ë ´, ìŒìˆ˜ ì…ë ¥ì— ë¯¼ê°
* **Leaky ReLU**: ìŒìˆ˜ ì…ë ¥ ë³´ì™„
* **ELU/SELU**: ê³ ê¸‰ ë„¤íŠ¸ì›Œí¬ì—ì„œ í™œìš©

---

## 3. ì •ê·œí™” ê¸°ë²•

| ê¸°ë²•        | ì„¤ëª…           | íš¨ê³¼         |
| --------- | ------------ | ---------- |
| L1        | ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©   | í¬ì†Œí•œ ëª¨ë¸ ìƒì„±  |
| L2        | ê°€ì¤‘ì¹˜ ì œê³±í•©      | ì¼ë°˜ì  ê³¼ì í•© ë°©ì§€ |
| Dropout   | ì¼ë¶€ ë‰´ëŸ° ë¬´ì‘ìœ„ ì œê±° | ì•™ìƒë¸” íš¨ê³¼ ìœ ì‚¬  |
| BatchNorm | ë°°ì¹˜ ë‹¨ìœ„ ì •ê·œí™”    | í•™ìŠµ ì•ˆì •ì„± ì¦ê°€  |

---

## 4. Early Stopping

* ê²€ì¦ ì •í™•ë„ê°€ **í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ**
* `patience` ì„¤ì •ì„ í†µí•´ n epoch ë™ì•ˆ ê°œì„  ì—†ì„ ê²½ìš° ì¤‘ë‹¨

---

## 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

| ìš”ì†Œ            | ì„¤ëª…            |
| ------------- | ------------- |
| Learning Rate | ê°€ì¥ ë¯¼ê°í•œ íŒŒë¼ë¯¸í„°   |
| Batch Size    | ë©”ëª¨ë¦¬-ì†ë„ ê· í˜•     |
| Layer ìˆ˜       | ë³µì¡ë„ì™€ í•™ìŠµ ì‹œê°„ ì¡°ì • |
| Hidden Unit ìˆ˜ | ê³¼ì†Œ/ê³¼ì í•© ì¡°ì ˆ ë„êµ¬  |
| Activation    | ë¹„ì„ í˜•ì„± ì„ íƒ       |

---

## 6. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. Dropoutì´ ì ìš©ëœ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** í•™ìŠµ ì‹œ ì¼ë¶€ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ , ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ í†µí•´ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ì•™ìƒë¸” íš¨ê³¼ê°€ ë°œìƒí•©ë‹ˆë‹¤.

---

### ë¬¸ì œ 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ ëª¨ë¸ì˜ í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì„ ê²½ìš° ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ë‚˜ìš”?

ğŸŸ© **ì •ë‹µ:** ìˆ˜ë ´ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì ¸ í•™ìŠµ ì‹œê°„ì´ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ê¸¸ì–´ì§

---

## 7. ì‹¤ìŠµ: Early Stoppingê³¼ Dropoutì„ ì´ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
y = df['Machine failure']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ëª¨ë¸ ìƒì„±
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early Stopping ì½œë°±
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train, y_train, epochs=50, batch_size=32,
                    validation_split=0.2, callbacks=[early_stop], verbose=1)

# ì„±ëŠ¥ í‰ê°€
y_pred = (model.predict(X_test) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss with EarlyStopping")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()
```

---

# ğŸ“˜ **4ì¥. DNNì„ í™œìš©í•œ IoT ë°ì´í„° ë¶„ë¥˜**


## 1. ë¶„ë¥˜(Classification) ë¬¸ì œ ì •ì˜

* ì…ë ¥: ë‹¤ì–‘í•œ ì„¼ì„œ íŠ¹ì„± (ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, í•´ì¶© ì‚¬ì§„ ë“±)
* ì¶œë ¥: ì´ë²¤íŠ¸ ë°œìƒ ì—¬ë¶€ (ì˜ˆ: ê¸°ê³„ ê³ ì¥ ì—¬ë¶€, í•´ì¶© ì¢…ë¥˜)

---

## 2. ë¶„ë¥˜ ëª¨ë¸ì—ì„œì˜ DNN êµ¬ì„±


| í•­ëª©         | ì´ì§„ ë¶„ë¥˜               | ë‹¤ì¤‘ ë¶„ë¥˜ (one-hot encoding)               |
| ---------- | ------------------- | -------------------------- |
| ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜   | 1                   | Nê°œ (í•´ì¶© ì¢…ë¥˜ ìˆ˜)               |
| ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ | Sigmoid             | Softmax         |
| ì†ì‹¤ í•¨ìˆ˜      | Binary Crossentropy | Categorical Crossentropy (one-hot encoding) |
| ì˜ˆì¸¡ ë°©ì‹      | 0 or 1    (0~1 ì‚¬ì´ í™•ë¥ )  | ê° ë¼ë²¨ë³„ë¡œ 0 ë˜ëŠ” 1              |
| ì˜ˆì‹œ         | ê¸°ê³„ ê³ ì¥ ì—¬ë¶€            | í•´ì¶© ì¢…ë¥˜    |

---

## 3. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

| ì§€í‘œ               | ì •ì˜                 | íŠ¹ì§•                   |
| ---------------- | ------------------ | -------------------- |
| Accuracy         | ì •í™•ë„ = (TP+TN)/(ì „ì²´) | í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œ ì™œê³¡ ê°€ëŠ¥      |
| Precision        | ì •ë°€ë„ = TP / (TP+FP) | ì˜ˆì¸¡ì´ ë§ì€ ë¹„ìœ¨            |
| Recall           | ì¬í˜„ìœ¨ = TP / (TP+FN) | ì‹¤ì œ ì–‘ì„± ì¤‘ ë§ì¶˜ ë¹„ìœ¨        |
| F1-score         | ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ ì¡°í™” í‰ê·      | ë¶ˆê· í˜• í´ë˜ìŠ¤ì— ì í•©          |
| Confusion Matrix | ì˜ˆì¸¡ vs ì‹¤ì œ ë¶„í¬í‘œ       | TP, TN, FP, FN êµ¬ì¡° í™•ì¸ |

- í˜¼ë™ í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„í•©ë‹ˆë‹¤:

| í•­ëª© | ì˜ë¯¸ |
|------|------|
| **True 1, Pred 1** | ì˜¬ë°”ë¥´ê²Œ ê³ ì¥ ì˜ˆì¸¡ (True Positive) |
| **True 0, Pred 0** | ì •ìƒ ì¥ë¹„ ì •í™• ì˜ˆì¸¡ (True Negative) |
| **True 0, Pred 1** | ì •ìƒì¸ë° ê³ ì¥ìœ¼ë¡œ ì˜ˆì¸¡ (False Positive) |
| **True 1, Pred 0** | ê³ ì¥ì¸ë° ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡ (False Negative) |
---

## 4. ì‹¤ì „ ì ìš©ì„ ìœ„í•œ ê³ ë ¤ ì‚¬í•­

* ì…ë ¥ ë³€ìˆ˜ì˜ **ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜**
* í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•œ ê²½ìš° **ê°€ì¤‘ì¹˜ ì¡°ì • ë˜ëŠ” ìƒ˜í”Œë§ ê¸°ë²•** ê³ ë ¤
* ì‹¤ì‹œê°„ IoT ì ìš©ì„ ìœ„í•œ **ëª¨ë¸ ê²½ëŸ‰í™”** ë° **ë°°í¬ ì „ëµ** í•„ìš”

---

## 5. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ ì ì ˆí•œ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** Sigmoid
ğŸ“ **ì„¤ëª…:** SigmoidëŠ” 0\~1 ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ë©°, í™•ë¥ ë¡œ í•´ì„í•  ìˆ˜ ìˆì–´ ì´ì§„ ë¶„ë¥˜ì— ì í•©í•©ë‹ˆë‹¤.

---
## 6. ì‹¤ìŠµ: ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ

### (1) ë¬¸ì œ
* **ìŠ¤ë§ˆíŠ¸ ì‹œí‹°ì˜ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ íŠ¹ì„± ê¸°ë°˜ ì¹¨ì… íƒì§€ ë° ê³µê²© ìœ í˜• ë¶„ë¥˜**í•˜ëŠ” ë¬¸ì œ.
* ë°ì´í„° ì…‹: ê³µê°œ ë°ì´í„°ì¸ [UNSW-NB15 ë„¤íŠ¸ì›Œí¬ ì¹¨ì… íƒì§€ ë°ì´í„°ì…‹](https://figshare.com/articles/dataset/UNSW_NB15_training-set_csv/29149946?file=54850502)
- ì‚¬ì´íŠ¸ì—ì„œ **UNSW_NB15_training-set.csv** íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ ,  Colab í™˜ê²½ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.


#### ğŸ“˜ `UNSW_NB15_training-set.csv` íŒŒì¼ì˜ ì£¼ìš” ì»¬ëŸ¼ë“¤


| ì»¬ëŸ¼ ì´ë¦„     | ì„¤ëª…                                 |
| --------- | ---------------------------------- |
| `id`      | ê° í–‰ì˜ ê³ ìœ  ì‹ë³„ì                        |
| `dur`     | ì„¸ì…˜ ì§€ì† ì‹œê°„ (ì´ˆ ë‹¨ìœ„)                    |
| `proto`   | ì‚¬ìš©ëœ ì „ì†¡ í”„ë¡œí† ì½œ (ì˜ˆ: TCP, UDP, ICMP ë“±)  |
| `service` | ëª©ì ì§€ ì„œë¹„ìŠ¤ ìœ í˜• (ì˜ˆ: HTTP, FTP, SMTP ë“±)  |
| `state`   | íŠ¸ë˜í”½ ì„¸ì…˜ì˜ ìƒíƒœ ì½”ë“œ (ì˜ˆ: FIN, REJ, INT ë“±) |
| `spkts`   | ì†ŒìŠ¤ì—ì„œ ë³´ë‚¸ íŒ¨í‚· ìˆ˜                       |
| `dpkts`   | ëª©ì ì§€ì—ì„œ ë³´ë‚¸ íŒ¨í‚· ìˆ˜                      |
| `sbytes`  | ì†ŒìŠ¤ê°€ ì „ì†¡í•œ ë°”ì´íŠ¸ ìˆ˜                      |
| `dbytes`  | ëª©ì ì§€ê°€ ì „ì†¡í•œ ë°”ì´íŠ¸ ìˆ˜                     |
| `rate`    | í‰ê·  íŒ¨í‚· ì „ì†¡ë¥                           |
| `sload`, `dload`   | ì†ŒìŠ¤/ëª©ì ì§€ì˜ ë°ì´í„° ì „ì†¡ ì†ë„ (bps)                |
| `stcpb`, `dtcpb`   | TCP ì´ˆê¸° ì‹œí€€ìŠ¤ ë²ˆí˜¸                          |
| `tcprtt`           | TCP ì™•ë³µ ì‹œê°„ (RTT)                        |
| `synack`, `ackdat` | TCP í•¸ë“œì…°ì´í¬ ê´€ë ¨ ì‹œê°„ ì •ë³´                     |
| `is_sm_ips_ports`  | ë™ì¼í•œ IP/í¬íŠ¸ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ì—¬ë¶€                    |
| `attack_cat`       | ê³µê²© ìœ í˜• ë¶„ë¥˜ (ì˜ˆ: Fuzzers, Exploits, DoS ë“±) |
| `label`            | ì´ì§„ ë ˆì´ë¸” (0 = ì •ìƒ, 1 = ê³µê²©)                |


* `attack_cat` ì»¬ëŸ¼ì€ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ í™œìš© ê°€ëŠ¥
* `label` ì»¬ëŸ¼ì€ ì´ì§„ ë¶„ë¥˜ìš© ë ˆì´ë¸”
* ëŒ€ë¶€ë¶„ì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ **ì •ê·œí™” ë˜ëŠ” ìŠ¤ì¼€ì¼ë§ í•„ìš”**
* `proto`, `service`, `state` ë“±ì€ **ë²”ì£¼í˜• ì¸ì½”ë”© í•„ìš”**


#### ğŸ“˜ ê³µê²© ìœ í˜•(`attack_cat` ì»¬ëŸ¼): 10ì¢…ë¥˜


| ì¸ë±ìŠ¤ | ê³µê²© ìœ í˜• (`attack_cat`)        | ì„¤ëª…                                                                  |
| --- | --------------------------- | ------------------------------------------------------------------- |
| 0   | **Analysis**                | í¬íŠ¸ ìŠ¤ìº”, íŒ¨í‚· ìº¡ì²˜, IDS íšŒí”¼ ë“± ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ë¶„ì„í•˜ê¸° ìœ„í•œ í™œë™. ì¢…ì¢… ì •ì°°ì´ë‚˜ ì¹¨íˆ¬ ì¤€ë¹„ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.         |
| 1   | **Backdoor**                | ì‹œìŠ¤í…œì— ëª°ë˜ ì ‘ê·¼í•˜ëŠ” ê²½ë¡œë¥¼ ë§Œë“œëŠ” ê³µê²© (ex: ì›ê²© ì…¸ ìƒì„±). ì‹œìŠ¤í…œì— ë¹„ì •ìƒì ìœ¼ë¡œ ì ‘ê·¼í•˜ê¸° ìœ„í•´ ì„¤ì¹˜ëœ ë¹„ë°€ ê²½ë¡œë¥¼ í†µí•´ ì¸ì¦ ì—†ì´ ì ‘ê·¼ ê°€ëŠ¥í•œ ê³µê²©ì…ë‹ˆë‹¤.               |
| 2   | **DoS (Denial of Service)** | ì„œë¹„ìŠ¤ ê±°ë¶€ ê³µê²©. ê³¼ë„í•œ íŠ¸ë˜í”½ìœ¼ë¡œ ìì›ì„ ì†Œëª¨ì‹œì¼œ ì •ìƒ ì„œë¹„ìŠ¤ ë°©í•´. ì„œë¹„ìŠ¤ ê±°ë¶€ ê³µê²©ìœ¼ë¡œ, ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ì‹œìŠ¤í…œì„ ë§ˆë¹„ì‹œì¼œ ì •ìƒ ì‚¬ìš©ìê°€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì§€ ëª»í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.         |
| 3   | **Exploits**                | ë³´ì•ˆ ì·¨ì•½ì ì„ ì´ìš©í•˜ì—¬ ì‹œìŠ¤í…œ ì œì–´ê¶Œì„ íšë“í•˜ëŠ” ê³µê²©. ì†Œí”„íŠ¸ì›¨ì–´, í•˜ë“œì›¨ì–´, ë˜ëŠ” OSì˜ ë³´ì•ˆ ì·¨ì•½ì ì„ ì•…ìš©í•˜ì—¬ ê¶Œí•œ ìƒìŠ¹ ë˜ëŠ” ëª…ë ¹ ì‹¤í–‰ ë“±ì„ ìœ ë„í•˜ëŠ” ê³µê²©ì…ë‹ˆë‹¤.      |
| 4   | **Fuzzers**                 | ì‹œìŠ¤í…œ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ì— ë¹„ì •ìƒì ì¸ ì…ë ¥ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì‚½ì…í•˜ì—¬ ì¶©ëŒì´ë‚˜ ì·¨ì•½ì ì„ ìœ ë°œí•˜ëŠ” ê³µê²©. ë‹¤ì–‘í•œ ì…ë ¥ ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì „ì†¡í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì·¨ì•½ì ì„ ë°œê²¬í•˜ë ¤ëŠ” ê¸°ë²•. ì¢…ì¢… ì‹œìŠ¤í…œ ì¶©ëŒì´ë‚˜ ì˜ˆì™¸ ë°œìƒì„ ìœ ë„í•©ë‹ˆë‹¤.   |
| 5   | **Generic**                 | ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ë°˜ì  ì•½ì ì„ í™œìš©í•˜ì—¬ ê³µê²©í•˜ëŠ” ë°©ì‹. ì˜ˆë¥¼ ë“¤ì–´, ë™ì¼í•œ í‚¤ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œì„ ë…¸ë¦½ë‹ˆë‹¤. |
| 6   | **Normal**                  | ì •ìƒì ì¸ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ìœ¼ë¡œ, ê³µê²©ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì‚¬ìš©ì í–‰ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.                         |
| 7   | **Reconnaissance**          | í¬íŠ¸ ìŠ¤ìºë‹, ë„¤íŠ¸ì›Œí¬ ë§¤í•‘ ë“±ê³¼ ê°™ì€ ì •ë³´ ìˆ˜ì§‘ í–‰ìœ„. ê³µê²©ì„ ìœ„í•œ ì‚¬ì „ ì¡°ì‚¬ì— í•´ë‹¹í•©ë‹ˆë‹¤.                |
| 8   | **Shellcode**               | ì‰˜ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë°”ì´ë„ˆë¦¬ ì½”ë“œë¡œ, ì£¼ë¡œ ì·¨ì•½ì ì„ ì•…ìš©í•´ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.             |
| 9   | **Worms**                   | ìì²´ ë³µì œ ë° í™•ì‚° ê¸°ëŠ¥ì„ ê°–ì¶˜ ì•…ì„±ì½”ë“œ (ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ê°ì—¼). ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ë³µì œ ë° í™•ì‚°ë˜ë©° ë‹¤ë¥¸ ì‹œìŠ¤í…œì„ ê°ì—¼ì‹œí‚¤ëŠ” ì•…ì„± í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.                    |



* `attack_cat` ì»¬ëŸ¼ì€ `LabelEncoder`ë¥¼ ì‚¬ìš©í•´ `0~9`ì˜ ì •ìˆ˜í˜• í´ë˜ìŠ¤ë¡œ ë³€í™˜ë¨.
* ì´í›„ `to_categorical()` ì²˜ë¦¬í•´ **ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì˜ íƒ€ê²Ÿ**ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.
* `Normal`ê³¼ ë‚˜ë¨¸ì§€ ê³µê²©ë“¤ì„ êµ¬ë¶„í•˜ë©´ **ì´ì§„ ë¶„ë¥˜**, ì „ì²´ë¥¼ êµ¬ë¶„í•˜ë©´ **ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ**ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.

---

### (2) ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œì˜ ë”¥ëŸ¬ë‹ êµ¬ì¡°

* ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜ = í´ë˜ìŠ¤ ìˆ˜ (ì˜ˆ: 10 ê°€ì§€ì˜ ê³µê²© ìœ í˜• â†’ 10ê°œ ë…¸ë“œ)
* ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜: `softmax`
* ì†ì‹¤ í•¨ìˆ˜: `categorical_crossentropy`

---
### (3) ì‹¤ìŠµ ì½”ë“œ

#### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë°ì´í„° ë¡œë“œ

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# CSV ë¡œë“œ
df = pd.read_csv('/content/UNSW_NB15_training-set.csv')
df.head()

df.info()

df.nunique()

df.isnull().sum()

df.isin([np.inf, -np.inf]).sum()
```

---

#### 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬

```python

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬: idì™€  label ë¶ˆí•„ìš”
X = df.drop(columns=['id', 'attack_cat', 'label'])
y = df['attack_cat']

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
#X.replace([np.inf, -np.inf], np.nan, inplace=True)
#X.fillna(0, inplace=True)

# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# ë ˆì´ë¸” ì¸ì½”ë”© 
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
class_labels = label_encoder.classes_
print(class_labels)

#  ì›-í•« ì¸ì½”ë”©
y_onehot = to_categorical(y_encoded)

```

---

#### 3. í›ˆë ¨Â·ê²€ì¦ ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§

```python

# ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
X_train, X_test, y_train, y_test = train_test_split(
		X, y_onehot, test_size=0.2, stratify=y_encoded)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


```

---

#### 4. MLP ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•

```python

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(y_onehot.shape[1], activation='softmax')
])

model.compile(optimizer='adam', 
		loss='categorical_crossentropy', 
		metrics=['accuracy'])
```

---

#### 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

```python


## EarlyStopping + ModelCheckpoint
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint("unsw_nb15_model.h5", save_best_only=True, monitor='val_loss')

# í•™ìŠµ
history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=64,
                    validation_split=0.2, 
		    callbacks=[early_stop, checkpoint], verbose=1)
# ì˜ˆì¸¡ ë° í‰ê°€
y_pred_proba = model.predict(X_test_scaled)
y_pred = np.argmax(y_pred_proba, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred, target_names=class_labels))

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
```

---

#### 6. í•™ìŠµ ê³¡ì„  ì‹œê°í™”

```python
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy over Epochs")
plt.legend()
plt.grid(True)
plt.show()
```

----
## 7. ë‹¤ì¤‘ë¶„ë¥˜ ê°™ì§€ë§Œ ë‹¤ì¤‘ë¶„ë¥˜ê°€ ì•„ë‹Œ ë©€í‹° ë¼ë²¨ ë¶„ë¥˜ ë¬¸ì œ

Ai4I 2020 Predictive Maintenance Datasetì—ì„œ ê³ ì¥ì˜ **ì›ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ** 


> ë‹¤ì¤‘ë¶„ë¥˜ì˜ ê²½ìš° ì¶œë ¥ì¸µì— ìˆëŠ” ë‰´ëŸ° ì¤‘ ì˜¤ì§ í•œ ê°œì˜ ë‰´ëŸ°ë§Œ ì¶œë ¥ '1'ì´ ë‚˜ì˜µë‹ˆë‹¤.
> ê·¸ëŸ¬ë‚˜, ê¸°ê³„ ê³ ì¥ ë¬¸ì œëŠ” ì´ ë¬¸ì œëŠ” **Multi-label classification**ì…ë‹ˆë‹¤.
> ì¦‰, í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ê³ ì¥ ìœ í˜•ì´ ë™ì‹œì— ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œê°€ 1ì„ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” **ë‹¤ì¤‘ ê³ ì¥ ë™ì‹œ ë°œìƒ ê°€ëŠ¥ì„±**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
> ì´ëŸ° ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ êµ¬ì„± ì½”ë“œê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.
---

### (1) ë©€í‹° ë¼ë²¨ ë¶„ë¥˜ì—ì„œì˜ ë”¥ëŸ¬ë‹ êµ¬ì¡°

* ì¶œë ¥ì¸µ ë…¸ë“œ ìˆ˜ = í´ë˜ìŠ¤ ìˆ˜ (ì˜ˆ: 5ê°œ ê³ ì¥ ì›ì¸ â†’ 5ê°œ ë…¸ë“œ)
* ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜: `sigmoid`
* ì†ì‹¤ í•¨ìˆ˜: `binary_crossentropy`


| ê³ ì¥ ìœ í˜• | ì˜ˆì¸¡ ê°’ (ì¶œë ¥ ë…¸ë“œ) | ì˜ë¯¸              |
| ----- | ------------ | --------------- |
| TWF   | 1            | ê³µêµ¬ ë§ˆëª¨ë¡œ ì¸í•œ ê³ ì¥ ë°œìƒ |
| HDF   | 0            | íˆí„° ê³ ì¥ ì—†ìŒ        |
| ...   | ...          | ...             |

* í•œ ì¥ë¹„ì— ì—¬ëŸ¬ ê°œì˜ ë…¸ë“œê°€ 1ì„ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” **ë‹¤ì¤‘ ê³ ì¥ ë™ì‹œ ë°œìƒ ê°€ëŠ¥ì„±**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

---

### (2) ì‹¤ìŠµ ì½”ë“œ: ë‹¤ì¤‘ ê³ ì¥ ì›ì¸ ë¶„ë¥˜

| ë‹¨ê³„         | ë‚´ìš©                                    |
| ---------- | ------------------------------------- |
| **ì…ë ¥ ë°ì´í„°** | Type, ì˜¨ë„, íšŒì „ ì†ë„, í† í¬, ë§ˆëª¨ ì‹œê°„            |
| **ì¶œë ¥ (y)** | ë‹¤ì¤‘ ì´ì§„ ê³ ì¥ ìœ í˜• (TWF, HDF, PWF, OSF, RNF) |
| **ëª¨ë¸ êµ¬ì¡°**  | 128 â†’ 64 â†’ 5 (sigmoid)                |
| **ì†ì‹¤ í•¨ìˆ˜**  | `binary_crossentropy` (ë‹¤ì¤‘ ì´ì§„ íƒ€ê²Ÿìš©)     |
| **í™œì„±í™” í•¨ìˆ˜** | ReLU + sigmoid ì¶œë ¥                     |
| **ì˜ˆì¸¡ í•´ì„**  | ê° ê³ ì¥ ìœ í˜•ë³„ 0.5 ì„ê³„ê°’ ì ìš©                   |
| **ì„±ëŠ¥ í‰ê°€**  | ê° ê³ ì¥ ìœ í˜•ë³„ `classification_report` ì¶œë ¥   |



```python
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam


# 2. ë°ì´í„° ë¡œë“œ
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
df = pd.read_csv(url)

# 3. ë°ì´í„° ì „ì²˜ë¦¬
df.drop(['UDI', 'Product ID'], axis=1, inplace=True)
 # ê³ ì¥ ì •ë³´ê°€ ì—†ëŠ” í–‰(ëª¨ë“  ì—´ì´ 0) ì„ ì‚­ì œ
df = df[df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']].sum(axis=1) > 0]
df['Type'] = df['Type'].map({'H': 0, 'L': 1, 'M': 2})

X = df[['Type', 'Air temperature [K]', 'Process temperature [K]',
        'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
X.columns = ['Type', 'AirTemp', 'ProcTemp', 'RotSpeed', 'Torque', 'ToolWear']

y = df[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']]

# 4. ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

y

y.sum()

y_test

y_test.sum()


```

```python
# 5. ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.3),
    Dense(64, activation='relu'),
        Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(5, activation='sigmoid')
])


model.compile(optimizer=Adam(learning_rate = 0.002), 
              loss='binary_crossentropy', metrics=['accuracy'])

# 6. ì½œë°± ì„¤ì • (EarlyStopping & ModelCheckpoint)
early_stop = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True, verbose=1)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)

# 7. í•™ìŠµ
history = model.fit(X_train, y_train,
                    epochs=150,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stop, checkpoint],
                    verbose=1)

# 8. ëª¨ë¸ ì˜ˆì¸¡
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# 9. ê° í´ë˜ìŠ¤ë³„ í‰ê°€ ë° í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
for i, col in enumerate(y.columns):
    print(f"\n[ê³ ì¥ ìœ í˜•: {col}]")
    print(classification_report(y_test[col], y_pred[:, i]))

    cm = confusion_matrix(y_test[col], y_pred[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Pred 0', 'Pred 1'],
                yticklabels=['True 0', 'True 1'])
    plt.title(f'Confusion Matrix: {col}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.tight_layout()
    plt.show()

# 10. í•™ìŠµ ì •í™•ë„ ì‹œê°í™”
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Multi-label Classification Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

```

---

# ğŸ“˜ **5ì¥. DNNì„ í™œìš©í•œ íšŒê·€**

## 1. íšŒê·€ ë¬¸ì œì˜ ì •ì˜

* ì…ë ¥: ì„¼ì„œ íŠ¹ì„± (ì˜¨ë„, í† í¬, íšŒì „ìˆ˜ ë“±)
* ì¶œë ¥: **ì—°ì†ê°’** (ì˜ˆ: ê³ ì¥ ì‹œì ê¹Œì§€ ë‚¨ì€ ì‹œê°„, ë§ˆëª¨ ì‹œê°„ ë“±)

---

## 2. ë¶„ë¥˜ì™€ íšŒê·€ì˜ ì£¼ìš” ì°¨ì´

| í•­ëª©         | ë¶„ë¥˜                       | íšŒê·€                    |
| ---------- | ------------------------ | --------------------- |
| ì¶œë ¥ê°’        | ë²”ì£¼ (0, 1, ë‹¤ì¤‘ í´ë˜ìŠ¤)        | ì—°ì†í˜• ìˆ˜ì¹˜                |
| ì¶œë ¥ì¸µ ë…¸ë“œ     | 1ê°œ ì´ìƒ (sigmoid, softmax) | 1ê°œ (ì„ í˜•)               |
| ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ | sigmoid/softmax          | **None (Linear)**     |
| ì†ì‹¤ í•¨ìˆ˜      | crossentropy             | **MSE / MAE / Huber** |

---

## 3. íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

| ì§€í‘œ                        | ìˆ˜ì‹                                 | í•´ì„                   |   |               |
| ------------------------- | ---------------------------------- | -------------------- | - | ------------- |
| MSE (Mean Squared Error)  | $\frac{1}{n} \sum (y - \hat{y})^2$ | ì˜¤ì°¨ ì œê³± í‰ê·  (ë¯¼ê°)        |   |               |
| MAE (Mean Absolute Error) | ( \frac{1}{n} \sum                 | y - \hat{y}          | ) | ì ˆëŒ€ ì˜¤ì°¨ í‰ê·  (ê°•ê±´) |
| RMSE                      | $\sqrt{MSE}$                       | í•´ì„ ìš©ì´                |   |               |
| RÂ² Score                  | $1 - \frac{SS_{res}}{SS_{tot}}$    | ì„¤ëª…ë ¥ ì§€í‘œ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ) |   |               |

---

## 4. ë³µìŠµ ë¬¸ì œ

### ë¬¸ì œ 1. íšŒê·€ ë¬¸ì œì—ì„œ ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ ì ì ˆí•œ ê²ƒì€?

ğŸŸ© **ì •ë‹µ:** ì—†ìŒ (ì„ í˜•)
ğŸ“ **ì„¤ëª…:** íšŒê·€ì—ì„œëŠ” ì—°ì†ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ë¯€ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

---

### ë¬¸ì œ 2. MSEì™€ MAEì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

ğŸŸ© **ì •ë‹µ:** MSEëŠ” ì˜¤ì°¨ ì œê³±ì„ ì‚¬ìš©í•˜ì—¬ í° ì˜¤ì°¨ì— ë¯¼ê°í•˜ê³ , MAEëŠ” ì ˆëŒ€ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì— ê°•ê±´í•©ë‹ˆë‹¤.

---

### ë¬¸ì œ 3. ë‹¤ìŒ ì¤‘ RÂ² Scoreê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”?

ğŸŸ© **ì •ë‹µ:** ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ê³  ì˜ˆì¸¡ë ¥ì´ ë†’ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

---

## 5. ì‹¤ìŠµ: **DNN íšŒê·€ ê¸°ë°˜ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡**

### (1) ì£¼ìš” ë³€ê²½

| í•­ëª© | ì„ í˜• íšŒê·€ | MLP íšŒê·€ |
|------|------------|------------|
| ëª¨ë¸ | `LinearRegression()` | `Keras Sequential` |
| ì†ì‹¤ í•¨ìˆ˜ | MSE | MSE |
| ì¶œë ¥ì¸µ | ì„ í˜• | ì„ í˜• (`Dense(1)`) |
| í‰ê°€ | RMSE, RÂ² | ë™ì¼ |

### (2) ì ìš© ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì ìš© ë‚´ìš© |
|------|-----------|
| **ì •ê·œí™”** | `kernel_regularizer=l2(0.001)` |
| **Dropout** | ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ 30%, ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ 20% |
| **EarlyStopping** | `patience=10`, `restore_best_weights=True` |

### (3) ì‹¤ìŠµ ì½”ë“œ

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping

# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
climate_path = "/content/GreenhouseClimate.csv"
prod_path = "/content/Production.csv"

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
climate = pd.read_csv(climate_path)
climate['Time'] = pd.to_datetime(climate['Time'], unit='D', origin='1900-01-01')

production = pd.read_csv(prod_path)
production['Time'] = pd.to_datetime(production['Time'], unit='D', origin='1900-01-01')

# 3. í•„ìš”í•œ ë³€ìˆ˜ë§Œ ì¶”ì¶œ
climate = climate[['Time', 'Tair', 'Rhair', 'CO2air', 'Tot_PAR', 'Cum_irr']]
production = production[['Time', 'ProdA']]  # ëª©í‘œ: Class A ìˆ˜í™•ëŸ‰

# 4. ì‹œê°„ ë‹¨ìœ„ í‰ê·  (í•˜ë£¨ ë‹¨ìœ„ë¡œ)
climate_indexed = climate.set_index('Time') 
production_indexed = production.set_index('Time')

numerical_cols = ['Tair', 'Rhair', 'CO2air', 'Tot_PAR', 'Cum_irr']
for col in numerical_cols:
    climate_indexed[col] = pd.to_numeric(climate_indexed[col], errors='coerce')

climate_daily = climate_indexed[numerical_cols].resample('D').mean().reset_index()
production_daily = production_indexed.resample('D').sum().reset_index()

# 5. ë³‘í•© ë° ê²°ì¸¡ì¹˜ ì œê±°
df = pd.merge(climate_daily, production_daily, on='Time')
df.dropna(inplace=True)

# 6. X, y ë¶„ë¦¬
X = df[numerical_cols]
y = df['ProdA']

# 7. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

```python
# 9. ëª¨ë¸ êµ¬ì„± (L2 ì •ê·œí™” + Dropout í¬í•¨)
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],),
          kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.2),
    Dense(1)  # íšŒê·€ ë¬¸ì œ: ì„ í˜• ì¶œë ¥
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# 10. EarlyStopping ì½œë°± ì„¤ì •
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# 11. ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train_scaled, y_train,
                    epochs=100,
                    batch_size=16,
                    validation_split=0.2,
                    callbacks=[early_stop],
                    verbose=0)
```

```python

# 12. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test_scaled).flatten()
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f" RMSE (MLP): {rmse:.2f}")
print(f" RÂ² Score (MLP): {r2:.2f}")

# 13. ì˜ˆì¸¡ vs ì‹¤ì œê°’ ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(y_test.values, label='Ground Truth')
plt.plot(y_pred, label='Predicted (MLP)', linestyle='--')
plt.title("Tomato Production Prediction (MLP + Reg + Dropout)")
plt.xlabel("Sample Index")
plt.ylabel("Production (kg/mÂ²)")
plt.legend()
plt.grid()
plt.show()

# 14. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title("MAE History with EarlyStopping")
plt.xlabel("Epoch")
plt.ylabel("MAE")
plt.legend()
plt.grid()
plt.show()

```

---

>>>>>>> 6759c28b4288f0c35ae1f2157cef39a269f336a9
